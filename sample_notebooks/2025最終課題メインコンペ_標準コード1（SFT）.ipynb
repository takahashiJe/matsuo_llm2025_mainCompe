{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "58cb53dee33b4ab8bf7560b019bc2da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ace2316866a4faa98205ec0def0eee1",
              "IPY_MODEL_56b0271afd874e689b45baed8eb9580b",
              "IPY_MODEL_3f53330b16ba4a58bc9f617e0099aa40",
              "IPY_MODEL_6a70f0412944416994a8b6145ad3766c",
              "IPY_MODEL_df3b8450eb8842bd9cf805c248df7036"
            ],
            "layout": "IPY_MODEL_ca84c316245a4eaeba774374c4f82b15"
          }
        },
        "1ace2316866a4faa98205ec0def0eee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f440d2885864ccfad2a0ff0b9a91d09",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_971117453ecd4f94aa16acccc8912c5b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "56b0271afd874e689b45baed8eb9580b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_eda0032f5db64f8186eb75ff8f1fa88a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1b013d538a4a4d17849fcedb15965cda",
            "value": ""
          }
        },
        "3f53330b16ba4a58bc9f617e0099aa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_911238ec235f44fa87f96a41e42a961a",
            "style": "IPY_MODEL_0fb19b7d96644ed78e248ab1d08bdff9",
            "value": true
          }
        },
        "6a70f0412944416994a8b6145ad3766c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b21268a4476e442fa68650ddfe4949bf",
            "style": "IPY_MODEL_0ac1930c6f7e421e8485290c5121a60f",
            "tooltip": ""
          }
        },
        "df3b8450eb8842bd9cf805c248df7036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abe52adb57bf49bb80243aa4bf51d4fb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ccd7a78422e44ba5ad8b253544d08112",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ca84c316245a4eaeba774374c4f82b15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0f440d2885864ccfad2a0ff0b9a91d09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "971117453ecd4f94aa16acccc8912c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eda0032f5db64f8186eb75ff8f1fa88a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b013d538a4a4d17849fcedb15965cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "911238ec235f44fa87f96a41e42a961a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fb19b7d96644ed78e248ab1d08bdff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b21268a4476e442fa68650ddfe4949bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ac1930c6f7e421e8485290c5121a60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "abe52adb57bf49bb80243aa4bf51d4fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd7a78422e44ba5ad8b253544d08112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆSFTï¼ˆUnsloth / Colab T4ï¼‰æ¨™æº–å­¦ç¿’ã‚³ãƒ¼ãƒ‰ï¼šå®Ÿè¡Œã‚¬ã‚¤ãƒ‰\n",
        "\n",
        "æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**æ§‹é€ åŒ–å‡ºåŠ›ã‚’æ¸¬ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚³ã‚¢å‘ä¸Š**ã‚’ç›®çš„ã¨ã—ã¦ã€  \n",
        "å°å‹LLMï¼ˆQwen3-4B Instruct-2507ï¼‰ã«å¯¾ã—ã¦ **SFTï¼ˆSupervised Fine-Tuningï¼‰** ã‚’è¡Œã†æ¨™æº–ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚\n",
        "\n",
        "å­¦ç¿’ã¯ **Unsloth + QLoRAï¼ˆ4bitï¼‰** ã‚’åˆ©ç”¨ã—ã€**Colab ç„¡æ–™ç‰ˆï¼ˆT4ï¼‰**ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã«ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚\n"
      ],
      "metadata": {
        "id": "M6vtQKTcQ8Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. ã“ã®ã‚³ãƒ¼ãƒ‰ãŒè¡Œã†ã“ã¨ï¼ˆæ¦‚è¦ï¼‰\n"
      ],
      "metadata": {
        "id": "v0xXIVtoRnQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ã“ã®ã‚³ãƒ¼ãƒ‰ã¯å¤§ãã3æ®µéšã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "1. **ç’°å¢ƒå›ºå®šï¼ˆä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å›ºå®šï¼‰**  \n",
        "   Colabã®ç’°å¢ƒå¤‰åŒ–ã«ã‚ˆã‚‹ä¸å…·åˆã‚’é¿ã‘ã‚‹ãŸã‚ã€numpy/transformers/trl/unslothç­‰ã‚’ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§æƒãˆã¾ã™ã€‚\n",
        "\n",
        "2. **SFTï¼ˆæ•™å¸«ã‚ã‚Šå¾®èª¿æ•´ï¼‰ã®å®Ÿè¡Œ**  \n",
        "   Hugging Face Hub ä¸Šã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã« LoRA ã‚¢ãƒ€ãƒ—ã‚¿ã‚’å·®ã—è¾¼ã¿ã€å­¦ç¿’ã—ã¾ã™ã€‚  \n",
        "   å­¦ç¿’ã®æå¤±ï¼ˆlossï¼‰ã¯ **assistant å‡ºåŠ›éƒ¨åˆ†ã ã‘**ã«ã‹ã‹ã‚‹è¨­è¨ˆã§ã™ï¼ˆstructured output ã‚’å­¦ç¿’ã•ã›ã‚„ã™ã„ï¼‰ã€‚\n",
        "\n",
        "3. **LoRAã‚¢ãƒ€ãƒ—ã‚¿ã®Hugging Faceã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**  \n",
        "   å­¦ç¿’ã§å¾—ã‚‰ã‚ŒãŸ LoRA ã®é‡ã¿ï¼ˆadapterï¼‰ã‚’ HF Hub ã«ä¿å­˜ã§ãã¾ã™ã€‚  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0bWkeEfIRZKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 2. å®Ÿè¡Œæ‰‹é †ï¼ˆæœ€çŸ­æ‰‹é †ï¼‰\n"
      ],
      "metadata": {
        "id": "56C03Zt2Rr3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 0: Colab ã®æº–å‚™\n",
        "- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ç¨®é¡ã‚’ **GPU** ã«å¤‰æ›´ã—ã€GPU ãŒ **T4** ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "- éå»ã®å®Ÿè¡Œã§ç’°å¢ƒãŒå£Šã‚Œã¦ã„ã‚‹å ´åˆã¯ **Runtime > Factory reset** ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n",
        "\n",
        "### Step 1: ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "- å…ˆé ­ã® `pip uninstall` â†’ `pip install` ã‚’ä¸Šã‹ã‚‰é †ã«å®Ÿè¡Œã—ã¾ã™ã€‚\n",
        "- å®Ÿè¡Œå¾Œã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¡¨ç¤ºãŒæƒ³å®šé€šã‚Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ï¼ˆ`unsloth import OK` ãŒå‡ºã‚‹ã“ã¨ï¼‰ã€‚\n",
        "\n",
        "### Step 2: Hugging Face ã¸ãƒ­ã‚°ã‚¤ãƒ³\n",
        "- `login()` ã‚’å®Ÿè¡Œã™ã‚‹ã¨ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚\n",
        "- å…¥åŠ›ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€**WRITEæ¨©é™**ã®ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚\n",
        "- â€»å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå…¬é–‹ãªã‚‰ãƒ­ã‚°ã‚¤ãƒ³ç„¡ã—ã§ã‚‚èª­ã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ãŒã€æ¨™æº–æ‰‹é †ã¨ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚\n",
        "\n",
        "### Step 3: å­¦ç¿’ã®å®Ÿè¡Œ\n",
        "- `main()` ãŒå‘¼ã°ã‚Œã€å­¦ç¿’ãŒé–‹å§‹ã—ã¾ã™ã€‚\n",
        "- å­¦ç¿’ä¸­ã« `[LabelStats:train]` ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€Œlosså¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¥µç«¯ã«ã‚¼ãƒ­ã«ãªã£ã¦ã„ãªã„ã‹ã€ã®å¥åº·è¨ºæ–­ã§ã™ã€‚\n",
        "\n",
        "### Step 4: å­¦ç¿’æˆæœç‰©ã®ç¢ºèªã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ã®huggingfaceã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "- å­¦ç¿’å¾Œã€`OUT_LORA_DIR` ã«ä»¥ä¸‹ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼ˆæœ€ä½é™ï¼‰ï¼š\n",
        "  - `adapter_config.json`\n",
        "  - `adapter_model.safetensors`ï¼ˆã¾ãŸã¯ `adapter_model.bin`ï¼‰\n",
        "  - tokenizer é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fsijt4sARZM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. å‡ºåŠ›ï¼ˆä½•ãŒç”Ÿæˆã•ã‚Œã‚‹ã‹ï¼‰\n",
        "\n"
      ],
      "metadata": {
        "id": "awaxrALwRxpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `OUT_LORA_DIR`ï¼ˆä¾‹ï¼š`/content/lora_structeval_t_qwen3_4b`ï¼‰ã«ã€\n",
        "  **LoRAã‚¢ãƒ€ãƒ—ã‚¿ï¼ˆå·®åˆ†é‡ã¿ï¼‰**ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚\n",
        "- ã“ã®ã‚¢ãƒ€ãƒ—ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã—ã¦æ¨è«–ã™ã‚‹ã“ã¨ã§ã€StructEval-T ã®ã‚¹ã‚³ã‚¢æ”¹å–„ã‚’ç‹™ã„ã¾ã™ã€‚\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PLhfZsZpRZPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 4. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¬æ˜\n"
      ],
      "metadata": {
        "id": "fp2GXhJyQ1NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4.1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦\n",
        "æœ¬ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã§ã™ï¼š\n",
        "\n",
        "- HF Dataset: `u-10bei/structured_data_with_cot_dataset_512_v2`  \n",
        "  https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "\n",
        "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€**æ§‹é€ åŒ–å‡ºåŠ›ï¼ˆCSV / JSON / XML / TOML / YAMLï¼‰**ã‚’ä¸­å¿ƒã¨ã—ãŸã€\n",
        "å½¢å¼å¤‰æ›ãƒ»æŠ½å‡ºã‚¿ã‚¹ã‚¯å‘ã‘ã®SFTãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚\n",
        "\n",
        "### 4.2 åéŒ²ä»¶æ•°ãƒ»split\n",
        "- Subset: `default`\n",
        "- Split: `train`\n",
        "- è¡Œæ•°ï¼š**ç´„ 3.65k rows**\n",
        "\n",
        "### 4.3 ã‚«ãƒ©ãƒ ï¼ˆåˆ—ï¼‰æ§‹é€ \n",
        "Viewerä¸Šã§ç¢ºèªã§ãã‚‹ä»£è¡¨çš„ãªã‚«ãƒ©ãƒ ã¯ä»¥ä¸‹ã§ã™ã€‚\n",
        "\n",
        "- `id`ï¼ˆæ–‡å­—åˆ—ï¼‰\n",
        "- `category`ï¼ˆã‚«ãƒ†ã‚´ãƒªï¼šè¤‡æ•°å€¤ï¼‰\n",
        "- `subcategory`ï¼ˆã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªï¼šè¤‡æ•°å€¤ï¼‰\n",
        "- `task`ï¼ˆã‚¿ã‚¹ã‚¯ç¨®åˆ¥ï¼šè¤‡æ•°å€¤ï¼‰\n",
        "- `seed`ï¼ˆç”Ÿæˆã‚„ç”±æ¥ã‚’ç¤ºã™è­˜åˆ¥å­ï¼‰\n",
        "- `messages`ï¼ˆ**OpenAI messageså½¢å¼ã®list**ï¼‰\n",
        "\n",
        "ç‰¹ã«é‡è¦ãªã®ãŒ `messages` ã§ã€å„ã‚µãƒ³ãƒ—ãƒ«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªå½¢ã§ã™ï¼š\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"role\": \"user\", \"content\": \"...æŒ‡ç¤ºã¨å…¥åŠ›...\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"...æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›...\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "tfLExuPZRZZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# å®Ÿè¡Œã‚³ãƒ¼ãƒ‰"
      ],
      "metadata": {
        "id": "iesWWPf3RZdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1:ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ],
      "metadata": {
        "id": "m9UNQU8fPoGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 0) ä¾å­˜é–¢ä¿‚ã®å›ºå®šï¼ˆColabã®â€œç’°å¢ƒãƒ–ãƒ¬â€å¯¾ç­–ï¼‰\n",
        "# ============================================================\n",
        "# Colabï¼ˆç„¡æ–™ç‰ˆï¼‰ã¯ã€ã‚ã‚‹æ—¥çªç„¶ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç‰ˆãŒå¤‰ã‚ã‚Šã€\n",
        "# ãã‚Œã¾ã§å‹•ã„ã¦ã„ãŸå­¦ç¿’ã‚³ãƒ¼ãƒ‰ãŒå£Šã‚Œã‚‹ã“ã¨ãŒé »ç¹ã«ã‚ã‚Šã¾ã™ã€‚\n",
        "# ãã®ãŸã‚ã€ã“ã®ã‚»ãƒ«ã§ã¯ã€Œä¸€åº¦å…¨éƒ¨æ¶ˆã™ â†’ äº’æ›ãŒç¢ºèªã§ããŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å…¥ã‚Œç›´ã™ã€\n",
        "# ã¨ã„ã†â€œå¼·åˆ¶çš„ãªå†ç¾æ€§ç¢ºä¿â€ã‚’ã—ã¦ã„ã¾ã™ã€‚\n",
        "#\n",
        "# â€»ErrorãŒå‡ºåŠ›ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ãŒã€ã€Œä½¿ç”¨ã—ãªã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ã«é–¢ã™ã‚‹ã‚¨ãƒ©ãƒ¼ã§ã‚ã‚Œã°ã€é–¢ä¿‚ãªãå‹•ä½œã—ã¾ã™ã€‚\n",
        "\n",
        "!pip -q uninstall -y numpy pandas datasets trl transformers accelerate peft unsloth unsloth-zoo bitsandbytes xformers\n",
        "!pip -q install \"numpy==2.0.2\" \"pandas==2.2.2\"\n",
        "!pip -q install \\\n",
        "  \"datasets==4.3.0\" \\\n",
        "  \"trl==0.24.0\" \\\n",
        "  \"transformers==4.56.2\" \\\n",
        "  \"accelerate==1.1.0\" \\\n",
        "  \"peft==0.13.2\" \\\n",
        "  \"bitsandbytes==0.45.0\"\n",
        "# unsloth / zoo ã‚’åŒç³»åˆ—ã§æƒãˆã‚‹ï¼ˆzooå´ã®è¦æ±‚ã«åˆã‚ã›ã‚‹ï¼‰\n",
        "# Unslothæœ¬ä½“ã¨ unsloth-zoo ã¯â€œã‚»ãƒƒãƒˆé‹ç”¨â€ãŒåŸºæœ¬ã§ã™ã€‚ç‰‡æ–¹ã ã‘ä¸Šã’ã‚‹ã¨å£Šã‚ŒãŒã¡ã§ã™ã€‚\n",
        "!pip -q install \"unsloth-zoo==2025.12.7\" \"unsloth==2025.12.7\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Bf7hi5wBSofx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ============================================================\n",
        "# 0.1) ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªï¼ˆâ€œå‹•ãã¯ãšâ€ã®çŠ¶æ…‹ã‹ã‚’ç›®è¦–ã§ç¢ºèªï¼‰\n",
        "# ============================================================\n",
        "# ã“ã“ã§æƒ³å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã‚ºãƒ¬ã¦ã„ã‚‹å ´åˆã€\n",
        "# å¾Œå·¥ç¨‹ã§åŸå› ä¸æ˜ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ç¢ºç‡ãŒä¸€æ°—ã«ä¸ŠãŒã‚Šã¾ã™ã€‚\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "import datasets, trl, transformers, torch\n",
        "\n",
        "print(\"numpy\", np.__version__)\n",
        "print(\"pandas\", pd.__version__)\n",
        "print(\"datasets\", datasets.__version__)\n",
        "print(\"trl\", trl.__version__)\n",
        "print(\"transformers\", transformers.__version__)\n",
        "print(\"torch\", torch.__version__)\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "print(\"unsloth import OK\")\n",
        "\n",
        "# æœŸå¾…å€¤ï¼š\n",
        "# numpy 2.0.2\n",
        "# pandas 2.2.2\n",
        "# datasets 4.3.0ï¼ˆã¾ãŸã¯ <4.4.0 ã§ 4.0.* / 4.1.0 ä»¥å¤–ï¼‰\n",
        "# trl 0.24.0ï¼ˆã¾ãŸã¯ 0.18.2ã€œ0.24.0 ã§ 0.19.0ä»¥å¤–ï¼‰\n",
        "# unsloth import OK\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 0) Install (single cell)\n",
        "# -----------------------------\n",
        "# NOTE:\n",
        "# - Colabã¯åˆæœŸçŠ¶æ…‹ãŒé »ç¹ã«å¤‰ã‚ã‚‹ãŸã‚ã€ãƒ”ãƒ³ç•™ã‚ã§å®‰å®šåŒ–ã—ã¾ã™ã€‚\n",
        "#   ã‚‚ã—ä¾å­˜é–¢ä¿‚ãŒå£Šã‚Œã¦ã„ã‚‹ç’°å¢ƒã§ã‚ã‚Œã°ã€Runtime > Factory reset ã‚’æ¨å¥¨ã€‚\n",
        "\n",
        "# ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ä¸Šã®ã€ŒæœŸå¾…å€¤ã€ã«ã‚‚ã—ãªã£ã¦ã„ãªã„å ´åˆã¯ã€ä¸‹è¨˜ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„ã€‚\n",
        "# !pip -q install -U \\\n",
        "#   \"numpy==2.0.2\" \\\n",
        "#   \"pandas==2.2.2\" \\\n",
        "#   \"datasets==4.3.0\" \\\n",
        "#   \"trl==0.24.0\" \\\n",
        "#   \"transformers==4.57.3\" \\åˆã¯ã€4.56.2\n",
        "#   \"accelerate==1.1.0\" \\\n",
        "#   \"peft==0.13.2\" \\\n",
        "#   \"bitsandbytes==0.45.0\" \\\n",
        "#   \"unsloth-zoo==2025.12.7\" \\\n",
        "#   \"unsloth==2025.12.7\" \\\n",
        "#   \"huggingface_hub\"\n"
      ],
      "metadata": {
        "id": "JHkB4LAjS1Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5287c61c-ae33-49f0-a866-120e29cd1e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy 2.0.2\n",
            "pandas 2.2.2\n",
            "datasets 4.3.0\n",
            "trl 0.24.0\n",
            "transformers 4.57.3\n",
            "torch 2.10.0+cu128\n",
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1458191610.py:17: UserWarning: WARNING: Unsloth should be imported before [trl, transformers] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "unsloth import OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: HuggingFace ãƒ­ã‚°ã‚¤ãƒ³\n",
        "\n",
        "Hugging Faceã«è‡ªåˆ†ã®ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜ã—ãŸã‚Šã€è¨­å®šã‚’å¤‰æ›´ã—ãŸã‚Šã™ã‚‹ã«ã¯ã€æ›¸ãè¾¼ã¿ç”¨ã®ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ãŒå¿…è¦ã§ã™ã€‚\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã§å–å¾—ã§ãã¾ã™ã€‚\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—1ï¼šè¨­å®šç”»é¢ã‚’é–‹ã\n",
        "- Hugging Face ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚(https://huggingface.co/)\n",
        "- ç”»é¢å³ä¸Šã®è‡ªåˆ†ã®ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚\n",
        "- ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ä¸­ã‹ã‚‰ ã€ŒSettingsã€ï¼ˆè¨­å®šï¼‰ã‚’é¸æŠã—ã¾ã™ã€‚\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—2ï¼šã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒšãƒ¼ã‚¸ã¸\n",
        "- å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«ã‚ã‚‹ ã€ŒAccess Tokensã€ ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—3ï¼šæ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œæˆã™ã‚‹\n",
        "- ç”»é¢ä¸­å¤®ã«ã‚ã‚‹ ã€Œ+ Create new tokenã€ ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚\n",
        "- è¨­å®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒé–‹ãã®ã§ã€ä»¥ä¸‹ã®2é …ç›®ã‚’å…¥åŠ›ãƒ»é¸æŠã—ã¾ã™ã€‚\n",
        "- Token Name: è‡ªåˆ†ãŒåˆ†ã‹ã‚Šã‚„ã™ã„åå‰ã‚’ä»˜ã‘ã¾ã™ï¼ˆä¾‹ï¼šmy-upload-token ãªã©ï¼‰ã€‚\n",
        "- Token type: ã“ã“ãŒä¸€ç•ªé‡è¦ã§ã™ï¼ä»Šå›ã¯ã€å­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¢ãƒ€ãƒ—ã‚¿ï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€å¿…ãš ã€ŒWriteã€ ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n",
        "- ä¸‹ã«ã‚ã‚‹ ã€ŒCreate tokenã€ ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦å®Œäº†ã§ã™ã€‚\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—4ï¼šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä¿å­˜ã™ã‚‹\n",
        "- ä½œæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®æ¨ªã«ã‚ã‚‹ ã‚³ãƒ”ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³ï¼ˆç´™ãŒé‡ãªã£ãŸãƒãƒ¼ã‚¯ï¼‰ ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚\n",
        "\n",
        "- ã‚³ãƒ”ãƒ¼ã—ãŸæ–‡å­—åˆ—ã¯ã€ãƒ¡ãƒ¢å¸³ãªã©ã«è²¼ã‚Šä»˜ã‘ã¦å¤§åˆ‡ã«ä¿ç®¡ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "âš ï¸ å¤§åˆ‡ãªæ³¨æ„ç‚¹\n",
        "ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€Œãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã€ã¨åŒã˜ã§ã™ï¼š ã“ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒä»–äººã«çŸ¥ã‚‰ã‚Œã‚‹ã¨ã€ã‚ãªãŸã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å‹æ‰‹ã«æ›¸ãæ›ãˆã‚‰ã‚Œã¦ã—ã¾ã†æã‚ŒãŒã‚ã‚Šã¾ã™ã€‚GitHubãªã©ã«ãã®ã¾ã¾è²¼ã‚Šä»˜ã‘ã¦å…¬é–‹ã—ãªã„ã‚ˆã†ã€ååˆ†æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "mi78wnsDP6Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# 1) HF login (once)\n",
        "# -----------------------------\n",
        "# Hugging Faceï¼ˆHFï¼‰ã¯ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚\n",
        "# ã“ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€ŒHF Hubä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã‚€ã€ã€Œå­¦ç¿’ã—ãŸLoRAã‚’HFã«ã‚¢ãƒƒãƒ—ã™ã‚‹ã€ãŸã‚ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚\n",
        "#\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import numpy as np, pandas as pd\n",
        "import datasets, trl, transformers, torch\n",
        "\n",
        "from huggingface_hub import login, HfApi\n",
        "login()  # Colab will prompt\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "SrjUvUoPP6i9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "58cb53dee33b4ab8bf7560b019bc2da8",
            "1ace2316866a4faa98205ec0def0eee1",
            "56b0271afd874e689b45baed8eb9580b",
            "3f53330b16ba4a58bc9f617e0099aa40",
            "6a70f0412944416994a8b6145ad3766c",
            "df3b8450eb8842bd9cf805c248df7036",
            "ca84c316245a4eaeba774374c4f82b15",
            "0f440d2885864ccfad2a0ff0b9a91d09",
            "971117453ecd4f94aa16acccc8912c5b",
            "eda0032f5db64f8186eb75ff8f1fa88a",
            "1b013d538a4a4d17849fcedb15965cda",
            "911238ec235f44fa87f96a41e42a961a",
            "0fb19b7d96644ed78e248ab1d08bdff9",
            "b21268a4476e442fa68650ddfe4949bf",
            "0ac1930c6f7e421e8485290c5121a60f",
            "abe52adb57bf49bb80243aa4bf51d4fb",
            "ccd7a78422e44ba5ad8b253544d08112"
          ]
        },
        "outputId": "e8b9ad00-8f87-4434-d813-4b5928cc81fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58cb53dee33b4ab8bf7560b019bc2da8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step3:å­¦ç¿’ã®å®Ÿè¡Œ"
      ],
      "metadata": {
        "id": "3YyMQDqNRI2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# 2) Training code\n",
        "# ============================================================\n",
        "# ã“ã“ã‹ã‚‰ãŒSFTæœ¬ä½“ã§ã™ã€‚\n",
        "# å¤§ã¾ã‹ãªæµã‚Œï¼š\n",
        "#  1) è¨­å®šå€¤ï¼ˆãƒ¢ãƒ‡ãƒ«åã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€LoRAè¨­å®šã€å­¦ç¿’ç‡ãªã©ï¼‰ã‚’èª­ã¿è¾¼ã‚€\n",
        "#  2) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’HFã‹ã‚‰å–å¾—ã—ã€å¿…è¦ãªå½¢ï¼ˆmessageså½¢å¼ï¼‰ã‚’æº€ãŸã™ã‚‚ã®ã ã‘æ®‹ã™\n",
        "#  3) tokenizerã§ã€Œå­¦ç¿’ã«ä½¿ã†ãƒ†ã‚­ã‚¹ãƒˆã€ã‚’ä½œã£ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ï¼ˆé«˜é€ŸåŒ–ï¼‰\n",
        "#  4) ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’4bitã§ãƒ­ãƒ¼ãƒ‰ã—ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’å·®ã—è¾¼ã‚€\n",
        "#  5) Trainerã§å­¦ç¿’ã‚’å›ã™\n",
        "#  6) LoRAã‚¢ãƒ€ãƒ—ã‚¿ã‚’ä¿å­˜ã™ã‚‹\n"
      ],
      "metadata": {
        "id": "QXdP-QwRQg3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼šQwen3-4B-Instruct-2507\n",
        "- GPUï¼šT4ï¼ˆç„¡æ–™Colabï¼‰ã§ã‚‚å›ã‚‹ã‚ˆã†ã«ã€ãƒ¡ãƒ¢ãƒªç¯€ç´„ã‚’å¼·ãæ„è­˜ã—ã¦ã„ã¾ã™ã€‚\n",
        "- å­¦ç¿’æ–¹å¼ï¼šQLoRAï¼ˆ4bitã§ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ã®ã¿å­¦ç¿’ï¼‰\n",
        "  - â€œå…¨éƒ¨ã®é‡ã¿â€ã‚’å­¦ç¿’ã™ã‚‹ã®ã§ã¯ãªãã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ï¼ˆè»½é‡å·®åˆ†ï¼‰ã ã‘ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
        "  - ãã®ãŸã‚ã€å­¦ç¿’å¾Œã«ä¿å­˜ã•ã‚Œã‚‹ã®ã‚‚ã€Œã‚¢ãƒ€ãƒ—ã‚¿ã€ä¸­å¿ƒã«ãªã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "QRNFLLFANPS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ä½¿ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "ä»Šå›ã€é‹å–¶ã«ãŠã„ã¦9ç¨®é¡ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ã¾ã—ãŸã€‚\n",
        "\n",
        "- 1-1. https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "- 1-2. https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v4\n",
        "- 1-3. https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v5\n",
        "- 1-4.\n",
        "https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512\n",
        "- 1-5.\n",
        "https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_v2\n",
        "- 1-6.\n",
        "https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset\n",
        "- 2-1. https://huggingface.co/datasets/daichira/structured-3k-mix-sft\n",
        "- 2-2. https://huggingface.co/datasets/daichira/structured-5k-mix-sft\n",
        "- 2-3. https://huggingface.co/datasets/daichira/structured-hard-sft-4k\n",
        "\n",
        " ã“ã®æ¨™æº–ã‚³ãƒ¼ãƒ‰ã§ã¯1-1ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€1-2ä»¥é™ã‚’ä½¿ç”¨ã—ã¦ã‚‚OKã§ã™ã€‚\n",
        " - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’1-1ä»¥å¤–ã«å¤‰æ›´ã›ãšã¨ã‚‚ã€å¾Œè¿°ã®ç’°å¢ƒå¤‰æ•°ï¼ˆ4.ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ï¼ˆä¿®äº†è¦ä»¶ã‚’æº€ãŸã™ï¼‰ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚\n",
        " - ã•ã‚‰ãªã‚‹æ€§èƒ½å‘ä¸Šã®ãŸã‚ã€ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è¿½åŠ ã§å‰å‡¦ç†ã‚’è¡Œã£ã¦ã‹ã‚‰å­¦ç¿’ã‚’è¡Œã£ã¦ã‚‚å·®ã—æ”¯ãˆã‚ã‚Šã¾ã›ã‚“ã€‚\n",
        "\n",
        " æ³¨æ„\n",
        "- ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã‚¹ã‚³ã‚¢ãŒä¸ŠãŒã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼\n",
        "- ã”è‡ªèº«ã§çµ„ã¿åˆã‚ã›ãŸã‚Šï¼Œã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ä½¿ç”¨ã—ã¦ã¿ã¦ãã ã•ã„ï¼\n",
        "- ãŸã ã—ï¼Œè©³ç´°è³‡æ–™ã«è¨˜è¼‰ã—ã¦ã‚ã‚‹ãƒ«ãƒ¼ãƒ«ã¯å®ˆã£ã¦ãã ã•ã„ï¼\n"
      ],
      "metadata": {
        "id": "2ok0cPOcRbTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n"
      ],
      "metadata": {
        "id": "NK6nsyrSQdyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š\n",
        "# -----------------------------\n",
        "# ä¸‹è¨˜ã®å€¤ã‚’æ›¸ãæ›ãˆã‚‹ã“ã¨ã§ã€ã‚³ãƒ¼ãƒ‰æœ¬ä½“ã‚’ç·¨é›†ã›ãšã«è¨­å®šã‚’å¤‰æ›´ã§ãã¾ã™ã€‚\n",
        "\n",
        "# 1. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–¢é€£\n",
        "os.environ[\"SFT_BASE_MODEL\"] = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "os.environ[\"SFT_DATASET_ID\"] = \"u-10bei/structured_data_with_cot_dataset_512_v2\"\n",
        "os.environ[\"SFT_OUT_LORA_DIR\"] = \"/content/lora_structeval_t_qwen3_4b\"\n",
        "\n",
        "# 2. å­¦ç¿’ã®åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "os.environ[\"SFT_SEED\"] = \"3407\"\n",
        "os.environ[\"SFT_VAL_RATIO\"] = \"0.05\"\n",
        "os.environ[\"SFT_MAX_SEQ_LEN\"] = \"512\"\n",
        "\n",
        "# 3. LoRA (ã‚¢ãƒ€ãƒ—ã‚¿) è¨­å®š\n",
        "os.environ[\"SFT_LORA_R\"] = \"64\"\n",
        "os.environ[\"SFT_LORA_ALPHA\"] = \"128\"\n",
        "os.environ[\"SFT_LORA_DROPOUT\"] = \"0\"\n",
        "os.environ[\"SFT_LORA_TARGET_MODULES\"] = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
        "\n",
        "# 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "os.environ[\"SFT_EPOCHS\"] = \"1\"\n",
        "os.environ[\"SFT_PER_DEVICE_TRAIN_BS\"] = \"2\"\n",
        "os.environ[\"SFT_PER_DEVICE_EVAL_BS\"] = \"2\"\n",
        "os.environ[\"SFT_GRAD_ACCUM\"] = \"8\"\n",
        "os.environ[\"SFT_LR\"] = \"1e-6\"\n",
        "os.environ[\"SFT_WARMUP_RATIO\"] = \"0.1\"\n",
        "os.environ[\"SFT_WEIGHT_DECAY\"] = \"0.05\"\n",
        "\n",
        "# 5. ã‚¹ãƒ†ãƒƒãƒ—ãƒ»ä¿å­˜è¨­å®š\n",
        "os.environ[\"SFT_MAX_STEPS\"] = \"-1\" # -1ã§ã‚¨ãƒãƒƒã‚¯ãƒ™ãƒ¼ã‚¹ã€‚å‹•ä½œç¢ºèªæ™‚ã¯ 10 ãªã©ã«ã€‚\n",
        "os.environ[\"SFT_LOGGING_STEPS\"] = \"10\"\n",
        "os.environ[\"SFT_EVAL_STEPS\"] = \"50\"\n",
        "os.environ[\"SFT_SAVE_STEPS\"] = \"100\"\n",
        "os.environ[\"SFT_SAVE_TOTAL_LIMIT\"] = \"2\"\n",
        "\n",
        "# 6. ç‰¹æ®Šå­¦ç¿’è¨­å®š (CoTãƒã‚¹ã‚¯ãƒ»ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)\n",
        "os.environ[\"SFT_MASK_COT\"] = \"1\" # \"1\" ã§æœ‰åŠ¹, \"0\" ã§ç„¡åŠ¹\n",
        "os.environ[\"SFT_OUTPUT_MARKERS\"] = \"Output:,OUTPUT:,Final:,Answer:,Result:,Response:\"\n",
        "os.environ[\"SFT_OUTPUT_LEARN_MODE\"] = \"after_marker\" # \"after_marker\" ã¾ãŸã¯ \"from_marker\"\n",
        "os.environ[\"SFT_USE_UPSAMPLING\"] = \"0\" # \"1\" ã§æœ‰åŠ¹, \"0\" ã§ç„¡åŠ¹  # ãƒ‡ãƒ¼ã‚¿2-1,2-2,2-3 å°‚ç”¨\n",
        "os.environ[\"SFT_UPSAMPLE_RULES\"] = '{\"xml_to_yaml\": 2.0}' # ä¾‹: '{\"json_to_xml\": 1.8, \"text_to_yaml\": 1.6}' # ãƒ‡ãƒ¼ã‚¿2-1,2-2,2-3 å°‚ç”¨\n",
        "\n",
        "print(\"ç’°å¢ƒå¤‰æ•°ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "z7t5SanBUJJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b9924a-a87f-48e1-899e-1e5994dbc5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç’°å¢ƒå¤‰æ•°ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOf9hrSaNl9x"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# 2.1) Config (env-overridable)\n",
        "# -----------------------------\n",
        "# â€œç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãã§ãã‚‹è¨­å®šâ€ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚\n",
        "# ã¤ã¾ã‚Šã€ã‚³ãƒ¼ãƒ‰ã‚’ç·¨é›†ã—ãªãã¦ã‚‚ã€Colabã®ç’°å¢ƒå¤‰æ•°ã‚’å¤‰ãˆã‚‹ã ã‘ã§\n",
        "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«åã€å­¦ç¿’ç‡ã€ã‚¨ãƒãƒƒã‚¯æ•°ãªã©ã‚’å¤‰æ›´ã§ãã‚‹è¨­è¨ˆã§ã™ã€‚\n",
        "#\n",
        "# ã“ã®è¨­è¨ˆã®ãƒ¡ãƒªãƒƒãƒˆï¼š\n",
        "# - â€œæ¨™æº–ã‚³ãƒ¼ãƒ‰â€ã¯åŒã˜ã¾ã¾ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘è©¦ã›ã‚‹ï¼ˆå†ç¾æ€§ãŒé«˜ã„ï¼‰\n",
        "\n",
        "def _getenv(name: str, default: str):\n",
        "    return os.environ.get(name, default)\n",
        "\n",
        "def _getenv_int(name: str, default: int) -> int:\n",
        "    try:\n",
        "        return int(os.environ.get(name, str(default)))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _getenv_float(name: str, default: float) -> float:\n",
        "    try:\n",
        "        return float(os.environ.get(name, str(default)))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "# å­¦ç¿’ã®â€œå‡ºç™ºç‚¹â€ã¨ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆ4Bï¼‰\n",
        "BASE_MODEL_ID = _getenv(\"SFT_BASE_MODEL\", \"Qwen/Qwen3-4B-Instruct-2507\")\n",
        "\n",
        "# å­¦ç¿’ã«ä½¿ã†SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆHF Hubä¸Šã«ç½®ã‹ã‚Œã¦ã„ã‚‹æƒ³å®šï¼‰\n",
        "DATASET_ID    = _getenv(\"SFT_DATASET_ID\", \"u-10bei/structured_data_with_cot_dataset_512_v2\")\n",
        "\n",
        "# å­¦ç¿’å¾Œã«ä¿å­˜ã•ã‚Œã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ã®å‡ºåŠ›å…ˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰\n",
        "OUT_LORA_DIR  = _getenv(\"SFT_OUT_LORA_DIR\", \"/content/lora_structeval_t_qwen3_4b\") # HFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿åã¨åˆã‚ã›ã‚‹\n",
        "\n",
        "SEED        = _getenv_int(\"SFT_SEED\", 3407)\n",
        "VAL_RATIO   = _getenv_float(\"SFT_VAL_RATIO\", 0.05)\n",
        "\n",
        "# 1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šæœ€å¤§ä½•ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§è¦‹ã‚‹ã‹ï¼ˆé•·ã„ã»ã©æƒ…å ±ã‚’è¦‹ã‚‰ã‚Œã‚‹ãŒã€GPUãƒ¡ãƒ¢ãƒªã¨æ™‚é–“ãŒå¢—ãˆã‚‹ï¼‰\n",
        "MAX_SEQ_LEN = _getenv_int(\"SFT_MAX_SEQ_LEN\", 512)\n",
        "\n",
        "# LoRA Configï¼ˆï¼â€œã©ã‚Œãã‚‰ã„ã®è¡¨ç¾åŠ›ã‚’æŒã¤å·®åˆ†ã‚’å­¦ç¿’ã™ã‚‹ã‹â€ï¼‰\n",
        "LORA_R       = _getenv_int(\"SFT_LORA_R\", 64)\n",
        "LORA_ALPHA   = _getenv_int(\"SFT_LORA_ALPHA\", 128)\n",
        "LORA_DROPOUT = _getenv_float(\"SFT_LORA_DROPOUT\", 0)\n",
        "LORA_TARGET_MODULES = (\n",
        "    _getenv(\"SFT_LORA_TARGET_MODULES\", \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\").split(\",\")\n",
        ")\n",
        "\n",
        "# Train hyperparamsï¼ˆå­¦ç¿’ã®åŸºæœ¬è¨­å®šï¼‰\n",
        "NUM_TRAIN_EPOCHS            = _getenv_int(\"SFT_EPOCHS\", 1)\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = _getenv_int(\"SFT_PER_DEVICE_TRAIN_BS\", 2)\n",
        "PER_DEVICE_EVAL_BATCH_SIZE  = _getenv_int(\"SFT_PER_DEVICE_EVAL_BS\", 2)\n",
        "\n",
        "# å‹¾é…ç´¯ç©ï¼šGPUã«ä¸€åº¦ã«è¼‰ã›ã‚‰ã‚Œã‚‹ãƒãƒƒãƒãŒå°ã•ã„æ™‚ã«ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã‚’è²¯ã‚ã¦â€œå¤§ãã„ãƒãƒƒãƒç›¸å½“â€ã«ã™ã‚‹\n",
        "GRAD_ACCUM                  = _getenv_int(\"SFT_GRAD_ACCUM\", 8)\n",
        "\n",
        "LR                          = _getenv_float(\"SFT_LR\", 1e-6)\n",
        "WARMUP_RATIO                = _getenv_float(\"SFT_WARMUP_RATIO\", 0.1)\n",
        "\n",
        "# Debug / quick check\n",
        "# MAX_STEPSã‚’å°ã•ãã™ã‚‹ã¨â€œå‹•ä½œç¢ºèªã ã‘â€ã®çŸ­æ™‚é–“å­¦ç¿’ãŒã§ãã¾ã™ï¼ˆæœ¬ç•ªã¯ -1 ã®ã¾ã¾ï¼‰\n",
        "MAX_STEPS        = _getenv_int(\"SFT_MAX_STEPS\", -1)\n",
        "LOGGING_STEPS    = _getenv_int(\"SFT_LOGGING_STEPS\", 10)\n",
        "EVAL_STEPS       = _getenv_int(\"SFT_EVAL_STEPS\", 50)\n",
        "SAVE_STEPS       = _getenv_int(\"SFT_SAVE_STEPS\", 100)\n",
        "SAVE_TOTAL_LIMIT = _getenv_int(\"SFT_SAVE_TOTAL_LIMIT\", 2)\n",
        "WEIGHT_DECAY     = _getenv_float(\"SFT_WEIGHT_DECAY\", 0.05)\n",
        "\n",
        "# Optional: upsampling rules\n",
        "# ç‰¹å®šã®ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹ï¼šé›£ã—ã„ã‚¿ã‚¹ã‚¯ï¼‰ã‚’â€œå¤šã‚ã«å­¦ç¿’ã•ã›ã‚‹â€ãŸã‚ã®ä»•çµ„ã¿ã€‚\n",
        "# æ¨™æº–ã§ã¯OFFã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "UPSAMPLE_ENABLE     = _getenv(\"SFT_USE_UPSAMPLING\", \"0\") in (\"1\",\"true\",\"True\")\n",
        "UPSAMPLE_RULES_JSON = _getenv(\"SFT_UPSAMPLE_RULES\", \"\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.2) Seed & Utils\n",
        "# -----------------------------\n",
        "# ä¹±æ•°ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ã‚’å›ºå®šã—ã¦ã€å†ç¾æ€§ã‚’æ‹…ä¿ã—ã¾ã™ã€‚\n",
        "# seedãŒåŒã˜ãªã‚‰ã€åŸå‰‡ã¨ã—ã¦åŒã˜åˆ†å‰²ãƒ»åŒã˜æŠ½å‡ºã«ãªã‚Šã‚„ã™ã„ã§ã™ã€‚\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(SEED)\n",
        "\n",
        "def ensure_openai_messages(ds: Dataset, msg_col: str = \"messages\") -> None:\n",
        "    # ãƒ‡ãƒ¼ã‚¿ãŒã€Œmessages: [{role, content}, ...]ã€å½¢å¼ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚\n",
        "    # ã“ã‚Œã¯ ChatGPTå½¢å¼ï¼ˆOpenAIã®Chat Completionså½¢å¼ã«ä¼¼ãŸï¼‰ã§ã€\n",
        "    # tokenizer.apply_chat_template ã§å®‰å…¨ã«æ–‡å­—åˆ—åŒ–ã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚\n",
        "    row0 = ds[0]\n",
        "    ex = row0.get(msg_col, None)\n",
        "    if not isinstance(ex, list):\n",
        "        raise ValueError(f\"Dataset must have list-style 'messages'. Got {type(ex)}\")\n",
        "\n",
        "def has_any_nonempty_assistant_turn(msgs: List[Dict[str, Any]]) -> bool:\n",
        "    # â€œassistantã®ç™ºè©±ãŒç©ºã˜ã‚ƒãªã„â€ã‚‚ã®ãŒ1å›ã§ã‚‚å«ã¾ã‚Œã‚‹ã‹ï¼Ÿ\n",
        "    # SFTã§ã¯ã€Œæ­£è§£ä¾‹ï¼ˆassistantã®å‡ºåŠ›ï¼‰ã€ãŒãªã„ã¨å­¦ç¿’ã§ããªã„ãŸã‚ã€‚\n",
        "    return any(m.get(\"role\") == \"assistant\" and str(m.get(\"content\", \"\")).strip() != \"\" for m in msgs)\n",
        "\n",
        "def ends_with_nonempty_assistant(ex: Dict[str, Any]) -> bool:\n",
        "    # æœ€å¾Œã®ã‚¿ãƒ¼ãƒ³ãŒ assistant ã®å›ç­”ã«ãªã£ã¦ã„ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã ã‘ã‚’ä½¿ã„ã¾ã™ã€‚\n",
        "    # ã“ã†ã—ã¦ãŠãã¨ã€Œæœ€å¾Œã®assistantã ã‘å­¦ç¿’ã™ã‚‹ï¼ˆassistant-only lossï¼‰ã€è¨­è¨ˆã¨ç›¸æ€§ãŒè‰¯ã„ã§ã™ã€‚\n",
        "    msgs = ex.get(\"messages\", [])\n",
        "    if not msgs or msgs[-1].get(\"role\") != \"assistant\":\n",
        "        return False\n",
        "    c = msgs[-1].get(\"content\", \"\")\n",
        "    return isinstance(c, str) and c.strip() != \"\"\n",
        "\n",
        "def shuffle_split(ds: Dataset, val_ratio: float, seed: int) -> Tuple[Dataset, Dataset]:\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ train/val ã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
        "    # valï¼ˆæ¤œè¨¼ï¼‰ã‚’æŒã¤ã“ã¨ã§ã€Œå­¦ç¿’ãŒé€²ã‚€ã»ã©æ€§èƒ½ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã‹ï¼éå­¦ç¿’ã—ã¦ã„ãªã„ã‹ã€ã‚’è¦‹ã‚‰ã‚Œã¾ã™ã€‚\n",
        "    ds_shuf = ds.shuffle(seed=seed)\n",
        "    n = len(ds_shuf)\n",
        "    n_val = max(1, int(round(n * val_ratio)))\n",
        "    return ds_shuf.select(range(n_val, n)), ds_shuf.select(range(n_val))\n",
        "\n",
        "def make_text_cache_builder(tokenizer):\n",
        "    # messageså½¢å¼ â†’ å®Ÿéš›ã«ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹â€œ1æœ¬ã®ãƒ†ã‚­ã‚¹ãƒˆâ€ã¸å¤‰æ›ã™ã‚‹é–¢æ•°ã‚’ä½œã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€Œãƒˆãƒ¼ã‚¯ãƒ³é•·ï¼ˆtruncationãªã—ï¼‰ã€ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚\n",
        "    #\n",
        "    # full_text  : ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆï¼ˆæ­£è§£ï¼‰ã¾ã§å«ã‚“ã å…¨æ–‡\n",
        "    # prefix_text: â€œæœ€å¾Œã®assistantã®ç›´å‰ã¾ã§â€ã®æ–‡ï¼ˆï¼ã“ã“ã‹ã‚‰assistantã‚’ç”Ÿæˆã•ã›ãŸã„ï¼‰\n",
        "    #\n",
        "    # ã“ã®2ã¤ã‚’æŒã¤ã“ã¨ã§ã€å¾Œã®collatorã§ã€Œassistantéƒ¨åˆ†ã ã‘ã‚’losså¯¾è±¡ã«ã™ã‚‹å¢ƒç•Œã€ã‚’è¨ˆç®—ã§ãã¾ã™ã€‚\n",
        "\n",
        "    def _build(batch):\n",
        "        full_out = []\n",
        "        prefix_out = []\n",
        "        full_len_out = []\n",
        "        prefix_len_out = []\n",
        "\n",
        "        for msgs in batch[\"messages\"]:\n",
        "            full = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
        "            prefix = tokenizer.apply_chat_template(msgs[:-1], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "            full_out.append(full)\n",
        "            prefix_out.append(prefix)\n",
        "\n",
        "            # é‡è¦ï¼šã“ã“ã§ truncation=False ã§ token é•·ã ã‘è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹\n",
        "            # add_special_tokens=False ã¯ã‚ãªãŸã®ç¾è¡Œè¨­è¨ˆã«åˆã‚ã›ã‚‹ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬å´ã§å¿…è¦ãƒˆãƒ¼ã‚¯ãƒ³ãŒå…¥ã‚‹æƒ³å®šï¼‰\n",
        "            full_ids = tokenizer(full, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "            prefix_ids = tokenizer(prefix, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "\n",
        "            full_len_out.append(len(full_ids))\n",
        "            prefix_len_out.append(len(prefix_ids))\n",
        "\n",
        "        return {\n",
        "            \"full_text\": full_out,\n",
        "            \"prefix_text\": prefix_out,\n",
        "            \"full_input_ids_len\": full_len_out,\n",
        "            \"prefix_input_ids_len\": prefix_len_out,\n",
        "        }\n",
        "\n",
        "    return _build\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.3) Collator (assistant-only loss)\n",
        "# -----------------------------\n",
        "# collatorã¯ã€Œç”Ÿã®ã‚µãƒ³ãƒ—ãƒ«ç¾¤ â†’ å­¦ç¿’ã«å¿…è¦ãªãƒ†ãƒ³ã‚½ãƒ«(input_ids/labelsç­‰)ã€ã«å¤‰æ›ã™ã‚‹éƒ¨å“ã§ã™ã€‚\n",
        "#\n",
        "# ã“ã“ãŒã“ã®å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã®â€œè¨­è¨ˆæ€æƒ³â€ã®æ ¸å¿ƒï¼š\n",
        "# - å…¥åŠ›ï¼ˆuser/systemï¼‰ã‚‚å«ã‚ã¦ãƒ¢ãƒ‡ãƒ«ã«ã¯èª­ã¾ã›ã‚‹\n",
        "# - ãŸã ã— lossï¼ˆèª¤å·®ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã®ã¯ assistant ã®å‡ºåŠ›éƒ¨åˆ†ã ã‘\n",
        "#\n",
        "# ã“ã‚Œã«ã‚ˆã‚Šï¼š\n",
        "# - ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸¸æš—è¨˜ã•ã›ã‚‹ã€æ–¹å‘ã«å­¦ç¿’ãŒå¼•ã£å¼µã‚‰ã‚Œã«ãã„\n",
        "# - â€œå›ç­”ã®å½¢å¼â€ã‚„â€œå‡ºåŠ›ã®æ­£ç¢ºã•â€ã«å­¦ç¿’ã®åŠ›ç‚¹ã‚’ç½®ãã¾ã™ã€‚\n",
        "\n",
        "# ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚‹ä»•æ§˜ã®é•ã„\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ1ï¼šOutput: ãŒ 100% ãªã®ã§ CoT ãƒã‚¹ã‚¯ãŒå¸¸ã«å‹•ãã€Outputæœ¬ä½“ã ã‘å­¦ç¿’\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ2ï¼šOutput: ç³»ãƒ©ãƒ™ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€CoTãƒã‚¹ã‚¯ã¯ç™ºå‹•ã›ãšã€â€œå‡ºåŠ›æœ¬ä½“â€ã‚’å­¦ç¿’\n",
        "\n",
        "# --- CoT mask settings (env overridable) ---\n",
        "MASK_COT = _getenv(\"SFT_MASK_COT\", \"1\") in (\"1\",\"true\",\"True\")\n",
        "OUTPUT_MARKERS = [s.strip() for s in _getenv(\n",
        "    \"SFT_OUTPUT_MARKERS\",\n",
        "    \"Output:,OUTPUT:,Final:,Answer:,Result:,Response:\"\n",
        ").split(\",\") if s.strip()]\n",
        "OUTPUT_LEARN_MODE = _getenv(\"SFT_OUTPUT_LEARN_MODE\", \"after_marker\")  # after_marker / from_marker\n",
        "\n",
        "@dataclass\n",
        "class AssistantOnlyCollatorCached:\n",
        "    tokenizer: Any\n",
        "    max_length: int = MAX_SEQ_LEN\n",
        "\n",
        "    def _find_subsequence(self, seq: List[int], sub: List[int]) -> int:\n",
        "        if not sub or len(sub) > len(seq):\n",
        "            return -1\n",
        "        for i in range(0, len(seq) - len(sub) + 1):\n",
        "            if seq[i:i+len(sub)] == sub:\n",
        "                return i\n",
        "        return -1\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        tok = self.tokenizer\n",
        "        full_texts   = [ex[\"full_text\"] for ex in batch]\n",
        "        prefix_texts = [ex[\"prefix_text\"] for ex in batch]\n",
        "\n",
        "        old_trunc = getattr(tok, \"truncation_side\", \"right\")\n",
        "        old_pad   = getattr(tok, \"padding_side\", \"right\")\n",
        "        tok.truncation_side = \"left\"\n",
        "        tok.padding_side    = \"right\"\n",
        "\n",
        "        try:\n",
        "            full_enc_tr = tok(\n",
        "                full_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                add_special_tokens=False,\n",
        "            )\n",
        "            input_ids = full_enc_tr[\"input_ids\"]\n",
        "            attention_mask = full_enc_tr[\"attention_mask\"]\n",
        "            labels = torch.full_like(input_ids, fill_value=-100)\n",
        "\n",
        "            full_ids_nt   = tok(full_texts,   return_tensors=None, padding=False, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
        "            prefix_ids_nt = tok(prefix_texts, return_tensors=None, padding=False, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "            marker_token_seqs = []\n",
        "            if MASK_COT and OUTPUT_MARKERS:\n",
        "                for m in OUTPUT_MARKERS:\n",
        "                    mid = tok(m, add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "                    if not mid:\n",
        "                        continue\n",
        "                    mid_nl = tok(m + \"\\n\", add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "                    mid_crlf = tok(m + \"\\r\\n\", add_special_tokens=False, truncation=False)[\"input_ids\"]\n",
        "                    marker_token_seqs.append((mid, mid_nl, mid_crlf))\n",
        "\n",
        "            for i in range(input_ids.size(0)):\n",
        "                trunc_left = max(0, len(full_ids_nt[i]) - self.max_length)\n",
        "                boundary = len(prefix_ids_nt[i]) - trunc_left\n",
        "                full_len_tr = int(attention_mask[i].sum().item())\n",
        "\n",
        "                # assistanté–‹å§‹ãŒè¦‹ãˆã¦ã„ãªã„ => å­¦ç¿’å¯¾è±¡å¤–ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰æ–¹é‡ã‚’ç¶­æŒï¼‰\n",
        "                if boundary <= 0 or boundary >= full_len_tr:\n",
        "                    continue\n",
        "\n",
        "                span_start = boundary\n",
        "                span_end   = full_len_tr\n",
        "\n",
        "                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šassistantå…¨ä½“ã‚’å­¦ç¿’ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ2ã¯ã“ã“ã«è½ã¡ã‚‹ï¼‰\n",
        "                learn_start = span_start\n",
        "\n",
        "                # CoTãƒã‚¹ã‚¯ï¼šOutput marker ãŒè¦‹ã¤ã‹ã£ãŸã¨ãã ã‘å­¦ç¿’é–‹å§‹ç‚¹ã‚’é€²ã‚ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ1ã§ç™ºå‹•ï¼‰\n",
        "                if MASK_COT and marker_token_seqs:\n",
        "                    visible_ids = input_ids[i, :full_len_tr].tolist()\n",
        "                    assistant_ids = visible_ids[span_start:span_end]\n",
        "\n",
        "                    best_out = None  # (out_pos, after_pos)\n",
        "                    for mid, mid_nl, mid_crlf in marker_token_seqs:\n",
        "                        # æ”¹è¡Œä»˜ãå„ªå…ˆ\n",
        "                        p = self._find_subsequence(assistant_ids, mid_nl)\n",
        "                        if p != -1:\n",
        "                            out_pos = span_start + p\n",
        "                            after_pos = out_pos + len(mid_nl)\n",
        "                        else:\n",
        "                            p = self._find_subsequence(assistant_ids, mid_crlf)\n",
        "                            if p != -1:\n",
        "                                out_pos = span_start + p\n",
        "                                after_pos = out_pos + len(mid_crlf)\n",
        "                            else:\n",
        "                                p = self._find_subsequence(assistant_ids, mid)\n",
        "                                if p == -1:\n",
        "                                    continue\n",
        "                                out_pos = span_start + p\n",
        "                                after_pos = out_pos + len(mid)\n",
        "\n",
        "                        if (best_out is None) or (out_pos < best_out[0]):\n",
        "                            best_out = (out_pos, after_pos)\n",
        "\n",
        "                    if best_out is not None:\n",
        "                        out_pos, after_pos = best_out\n",
        "                        if OUTPUT_LEARN_MODE == \"from_marker\":\n",
        "                            learn_start = out_pos\n",
        "                        else:\n",
        "                            learn_start = after_pos\n",
        "                        learn_start = max(span_start, min(learn_start, span_end))\n",
        "\n",
        "                if learn_start < span_end:\n",
        "                    labels[i, learn_start:span_end] = input_ids[i, learn_start:span_end]\n",
        "\n",
        "            labels[attention_mask == 0] = -100\n",
        "            return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "        finally:\n",
        "            tok.truncation_side = old_trunc\n",
        "            tok.padding_side    = old_pad\n",
        "\n",
        "import random, torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def filter_has_supervision(ds, collator):\n",
        "    keep = []\n",
        "    for i in range(len(ds)):\n",
        "        out = collator([ds[i]])\n",
        "        if (out[\"labels\"][0] != -100).sum().item() > 0:\n",
        "            keep.append(i)\n",
        "    return ds.select(keep)\n",
        "\n",
        "\n",
        "def count_all_masked(ds, collator, n=200, seed=3407):\n",
        "    rng = random.Random(seed)\n",
        "    n = min(n, len(ds))\n",
        "    idxs = [rng.randrange(0, len(ds)) for _ in range(n)]\n",
        "    all_masked = 0\n",
        "    for i in idxs:\n",
        "        out = collator([ds[i]])\n",
        "        labels = out[\"labels\"][0]\n",
        "        if (labels != -100).sum().item() == 0:\n",
        "            all_masked += 1\n",
        "    print(f\"[CHECK] all-masked samples in {n}: {all_masked} ({all_masked/max(1,n):.1%})\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.4) Optional upsampling\n",
        "# -----------------------------\n",
        "# upsamplingã¯ã€Œç‰¹å®šã®ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤šã‚ã«å­¦ç¿’ã•ã›ã‚‹ã€ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã§ã™ã€‚\n",
        "# ä¾‹ï¼š\n",
        "# - JSONã¯å¾—æ„ã ãŒYAMLã¯è‹¦æ‰‹ â†’ YAMLé–¢é€£ã‚µãƒ³ãƒ—ãƒ«ã‚’2å€ã«ã™ã‚‹\n",
        "# - ç‰¹å®šã®subcategoryãŒç‚¹æ•°ã«åŠ¹ã â†’ ãã“ã‚’åšãã™ã‚‹\n",
        "# ãŸã ã—ã€ã‚„ã‚Šã™ãã‚‹ã¨ä»–ãŒå¼±ããªã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰ã€‚\n",
        "# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªãŒæ‚ªã„ç­‰ã®åŸå› ã§ã€å´ã£ã¦æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
        "# ãã®å ´åˆã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¦³å¯Ÿã—ã€è¿½åŠ ã®å‰å‡¦ç†ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚‚å¤šã„ã§ã™ã€‚\n",
        "\n",
        "def apply_upsampling(train_ds: Dataset) -> Dataset:\n",
        "    if not UPSAMPLE_ENABLE or not UPSAMPLE_RULES_JSON:\n",
        "        return train_ds\n",
        "    try:\n",
        "        rules = json.loads(UPSAMPLE_RULES_JSON)\n",
        "        if not isinstance(rules, dict) or not rules:\n",
        "            return train_ds\n",
        "    except Exception:\n",
        "        return train_ds\n",
        "\n",
        "    packs = train_ds[\"subcategory\"] if \"subcategory\" in train_ds.column_names else [None]*len(train_ds)\n",
        "    pack_field = train_ds[\"pack\"] if \"pack\" in train_ds.column_names else [None]*len(train_ds)\n",
        "\n",
        "    w = []\n",
        "    for sub, pk in zip(packs, pack_field):\n",
        "        weight = 1.0\n",
        "        ssub = str(sub or \"\")\n",
        "        spk  = str(pk or \"\")\n",
        "        for pat, mult in rules.items():\n",
        "            try:\n",
        "                m = float(mult)\n",
        "            except Exception:\n",
        "                m = 1.0\n",
        "            if pat.startswith(\"pack:\"):\n",
        "                if spk == pat.split(\":\",1)[1]:\n",
        "                    weight *= max(0.0, m)\n",
        "            else:\n",
        "                if pat in ssub:\n",
        "                    weight *= max(0.0, m)\n",
        "        w.append(weight)\n",
        "\n",
        "    w = np.asarray(w, dtype=np.float64)\n",
        "    if (w <= 0).all() or w.sum() == 0:\n",
        "        return train_ds\n",
        "\n",
        "    p = w / w.sum()\n",
        "    n = len(train_ds)\n",
        "    idx = np.random.choice(np.arange(n), size=n, replace=True, p=p)\n",
        "    print(\"[UPSAMPLE] rules:\", rules)\n",
        "    return train_ds.select(idx.tolist())\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.5) Callback (monitor)\n",
        "# -----------------------------\n",
        "# å­¦ç¿’ä¸­ã®ãƒ‡ãƒãƒƒã‚°ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã™ã€‚\n",
        "# ã“ã“ã§ã¯ã€Œlabelsã®ã†ã¡ã€å®Ÿéš›ã«losså¯¾è±¡ã«ãªã£ã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³å‰²åˆã€ã‚’æ™‚ã€…è¡¨ç¤ºã—ã¾ã™ã€‚\n",
        "#\n",
        "# æ„å‘³ï¼š\n",
        "# - valid_ratio ãŒæ¥µç«¯ã«å°ã•ã„ â†’ â€œå­¦ç¿’ã—ã¦ã„ãªã„â€ã®ã¨åŒã˜ï¼ˆãƒ©ãƒ™ãƒ«ãŒã»ã¼ -100ï¼‰\n",
        "# - valid_ratio ãŒé©åº¦ã«ã‚ã‚‹ â†’ assistantéƒ¨åˆ†ã«ã—ã£ã‹ã‚ŠlossãŒä¹—ã£ã¦ã„ã‚‹\n",
        "#\n",
        "# åˆå­¦è€…å‘ã‘ã«è¨€ã†ã¨ï¼š\n",
        "# - ã“ã‚Œã¯â€œå­¦ç¿’ãŒã¡ã‚ƒã‚“ã¨åŠ¹ã„ã¦ã„ã‚‹ã‹ã®å¥åº·è¨ºæ–­â€ã§ã™ã€‚\n",
        "\n",
        "class LabelStatsCallback(TrainerCallback):\n",
        "    def __init__(self, dataset, collator, name=\"train\", every_n_steps=100):\n",
        "        self.dataset, self.collator, self.name, self.every_n_steps = dataset, collator, name, every_n_steps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if (state.global_step % self.every_n_steps) == 0:\n",
        "            batch = [self.dataset[random.randint(0, len(self.dataset)-1)] for _ in range(8)]\n",
        "            out = self.collator(batch)\n",
        "            valid = (out[\"labels\"] != -100).sum().item()\n",
        "            total = (out[\"attention_mask\"] == 1).sum().item()\n",
        "            print(f\"\\n[LabelStats:{self.name}] step={state.global_step} valid_ratio={valid/max(1,total):.4f}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2.6) Main\n",
        "# -----------------------------\n",
        "# å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
        "\n",
        "def main():\n",
        "    os.makedirs(OUT_LORA_DIR, exist_ok=True)\n",
        "\n",
        "    # if you used /content/your_id cache dirs etc, remove to avoid confusion\n",
        "    if os.path.exists(\"/content/your_id\"):\n",
        "        shutil.rmtree(\"/content/your_id\")\n",
        "\n",
        "    print(f\"[INFO] Loading dataset from HF Hub: {DATASET_ID}\")\n",
        "    ds_all = load_dataset(DATASET_ID, split=\"train\")\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ãƒã‚§ãƒƒã‚¯ï¼ˆmessagesãŒlistã§ã‚ã‚‹ã“ã¨ï¼‰\n",
        "    ensure_openai_messages(ds_all)\n",
        "\n",
        "    # å­¦ç¿’ã§ãã‚‹ã‚µãƒ³ãƒ—ãƒ«ã ã‘æ®‹ã™ï¼ˆassistantãŒç©ºãªã‚‰æ•™å¸«ä¿¡å·ãŒç„¡ã„ï¼‰\n",
        "    ds_all = ds_all.filter(lambda ex: has_any_nonempty_assistant_turn(ex[\"messages\"]))\n",
        "    ds_all = ds_all.filter(ends_with_nonempty_assistant)\n",
        "\n",
        "    # train/valåˆ†å‰²\n",
        "    train_ds, val_ds = shuffle_split(ds_all, VAL_RATIO, SEED)\n",
        "\n",
        "    # Optional: upsampling by ruleï¼ˆåˆ†å‰²å¾Œã«é©ç”¨ï¼‰\n",
        "    train_ds = apply_upsampling(train_ds)\n",
        "\n",
        "    print(\"[INFO] Loading base model:\", BASE_MODEL_ID)\n",
        "\n",
        "    # Unslothã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆ4bitãƒ­ãƒ¼ãƒ‰ã§çœãƒ¡ãƒ¢ãƒªï¼‰\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=BASE_MODEL_ID,\n",
        "        max_seq_length=MAX_SEQ_LEN,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Cache chat template rendersï¼ˆtokenizerãŒå¿…è¦ãªã®ã§ã“ã“ã§åˆã‚ã¦build_cacheã‚’ä½œã‚‹ï¼‰\n",
        "    build_cache = make_text_cache_builder(tokenizer)\n",
        "\n",
        "    train_ds = train_ds.map(build_cache, batched=True, num_proc=1, desc=\"Caching train\")\n",
        "    val_ds   = val_ds.map(build_cache,   batched=True, num_proc=1, desc=\"Caching val\")\n",
        "\n",
        "    # Attach LoRA\n",
        "    # ã“ã“ã§ã€Œå­¦ç¿’ã•ã‚Œã‚‹éƒ¨åˆ†ï¼ˆLoRAã‚¢ãƒ€ãƒ—ã‚¿ï¼‰ã€ã‚’ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ã—ã¾ã™ã€‚\n",
        "    # å­¦ç¿’å¯¾è±¡ã¯ LoRA ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘ã«ãªã‚Šã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å·¨å¤§ãªé‡ã¿ã¯å›ºå®šã•ã‚Œã¾ã™ã€‚\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=LORA_R,\n",
        "        target_modules=LORA_TARGET_MODULES,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=SEED,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Transformersã®å¼•æ•°åãŒãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§æºã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "    # ä»Šå›ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ eval_strategy ã‚’ä½¿ã„ã¾ã™ã€‚\n",
        "    args = TrainingArguments(\n",
        "        output_dir=OUT_LORA_DIR,\n",
        "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        learning_rate=LR,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=EVAL_STEPS,\n",
        "\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=SAVE_STEPS,\n",
        "        save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "\n",
        "        max_steps=MAX_STEPS,  # -1 => epoch-based\n",
        "\n",
        "        bf16=False,\n",
        "        fp16=True,            # T4å‘ã‘ï¼ˆT4ã¯bf16ãŒå¼±ã„ã®ã§fp16ã‚’ä½¿ã†ã®ãŒä¸€èˆ¬çš„ï¼‰\n",
        "\n",
        "        push_to_hub=False,\n",
        "        report_to=\"none\",\n",
        "\n",
        "        group_by_length=False,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    # assistant-only loss ã® collator ã‚’ä½¿ã†\n",
        "    collator = AssistantOnlyCollatorCached(tokenizer=tokenizer, max_length=MAX_SEQ_LEN)\n",
        "\n",
        "    # --- NaNå¯¾ç­–ï¼šall-maskedï¼ˆæ•™å¸«ãƒˆãƒ¼ã‚¯ãƒ³0ï¼‰ã‚’é™¤å»ã—ã¦è©•ä¾¡ã‚’å®‰å®šåŒ– ---\n",
        "    print(\"[INFO] Checking all-masked samples before filtering...\")\n",
        "    count_all_masked(val_ds, collator, n=len(val_ds), seed=SEED)\n",
        "\n",
        "    print(\"[INFO] Filtering train/val to remove all-masked samples...\")\n",
        "    train_ds = filter_has_supervision(train_ds, collator)\n",
        "    val_ds   = filter_has_supervision(val_ds, collator)\n",
        "\n",
        "    print(\"[INFO] New sizes:\", \"train =\", len(train_ds), \"val =\", len(val_ds))\n",
        "    print(\"[INFO] Checking all-masked samples after filtering...\")\n",
        "    count_all_masked(val_ds, collator, n=len(val_ds), seed=SEED)\n",
        "\n",
        "\n",
        "    # Trainerï¼ˆTransformersã®æ¨™æº–å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼‰\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        data_collator=collator,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # ç›£è¦–ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ï¼ˆå­¦ç¿’ãŒåŠ¹ã„ã¦ã„ã‚‹ã‹ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼‰\n",
        "    trainer.add_callback(LabelStatsCallback(train_ds, collator, name=\"train\", every_n_steps=LOGGING_STEPS))\n",
        "\n",
        "    print(\"[INFO] Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # å­¦ç¿’å¾Œã®ä¿å­˜ï¼šLoRAã‚¢ãƒ€ãƒ—ã‚¿ï¼†tokenizer\n",
        "    print(\"[INFO] Saving adapter & tokenizer...\")\n",
        "    model.save_pretrained(OUT_LORA_DIR)\n",
        "    tokenizer.save_pretrained(OUT_LORA_DIR)\n",
        "    print(f\"[INFO] Done. Saved to {OUT_LORA_DIR}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: å­¦ç¿’æˆæœç‰©ã®ç¢ºèªã¨ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ã®huggingfaceã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "å­¦ç¿’å¾Œã€OUT_LORA_DIR ã«ä»¥ä¸‹ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼ˆæœ€ä½é™ï¼‰ã®ã§ã€ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "- adapter_config.json\n",
        "- adapter_model.safetensorsï¼ˆã¾ãŸã¯ adapter_model.binï¼‰\n",
        "- tokenizer é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«\n",
        "<br>\n",
        "\n",
        "ä¸‹è¨˜ã«å¾“ã£ã¦\"README.md\"ã‚’è¨˜è¼‰ã—ã¦ã‹ã‚‰ã€HuggingFaceã«ã‚¢ãƒ€ãƒ—ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "nVAg0vIdVLg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### â‘  README.md ã®æ­£ã—ã„æ›¸ãæ–¹\n",
        "\n",
        "#### README.md ã®å½¹å‰²ï¼ˆæœ€é‡è¦ï¼‰\n",
        "\n",
        "Hugging Face ã§ã¯ **README.md = ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰**ã§ã™ã€‚\n",
        "ã€Œã“ã®LoRAã¯ä½•ã‚’å­¦ç¿’ã—ã€ã©ã†ä½¿ã„ã€ä½•ã«æ³¨æ„ã™ã¹ãã‹ã€ã‚’ **ç¬¬ä¸‰è€…ãŒå†åˆ©ç”¨ã§ãã‚‹æ°´æº–ã§èª¬æ˜ã™ã‚‹ç¾©å‹™**ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "README ãŒä¸ååˆ†ãªãƒ¢ãƒ‡ãƒ«ã¯ã€\n",
        "\n",
        "* OSSã¨ã—ã¦ä¸é©åˆ‡\n",
        "* å­¦ç¿’å†…å®¹ãŒä¸é€æ˜\n",
        "* ãƒ©ã‚¤ã‚»ãƒ³ã‚¹é•åãƒªã‚¹ã‚¯ã‚ã‚Š\n",
        "  ã¨è©•ä¾¡ã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "#### å¿…é ˆæ§‹æˆï¼ˆã“ã®é †ã§æ›¸ãã“ã¨ï¼‰\n",
        "\n",
        "##### 1. YAMLãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆï¼‰\n",
        "\n",
        "```yaml\n",
        "---\n",
        "base_model: Qwen/Qwen3-4B-Instruct-2507\n",
        "datasets:\n",
        "- u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "language:\n",
        "- en\n",
        "license: Apache-2.0\n",
        "library_name: peft\n",
        "pipeline_tag: text-generation\n",
        "tags:\n",
        "- qlora\n",
        "- lora\n",
        "- structured-output\n",
        "---\n",
        "```\n",
        "\n",
        "**ç†ç”±**\n",
        "\n",
        "* HFæ¤œç´¢ãƒ»åˆ†é¡ãƒ»å†ç¾æ€§ã«å¿…é ˆ\n",
        "* ç„¡ã„ã¨ã€Œå£Šã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã€æ‰±ã„ã«ãªã‚‹\n",
        "\n",
        "---\n",
        "\n",
        "##### 2. ãƒ¢ãƒ‡ãƒ«æ¦‚è¦ï¼ˆWhatï¼‰\n",
        "\n",
        "```md\n",
        "# qwen3-4b-structured-output-lora\n",
        "\n",
        "This repository provides a **LoRA adapter** fine-tuned from\n",
        "**Qwen3-4B-Instruct-2507** using **QLoRA (4-bit, Unsloth)**.\n",
        "\n",
        "This repository contains **LoRA adapter weights only**.\n",
        "The base model must be loaded separately.\n",
        "```\n",
        "\n",
        "**å¿…é ˆãƒã‚¤ãƒ³ãƒˆ**\n",
        "\n",
        "* ã€ŒLoRAã‚¢ãƒ€ãƒ—ã‚¿ã®ã¿ã€ã§ã‚ã‚‹ã“ã¨ã‚’æ˜è¨˜\n",
        "* ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«åã‚’æ˜ç¤º\n",
        "\n",
        "---\n",
        "\n",
        "##### 3. å­¦ç¿’ç›®çš„ãƒ»è¨­è¨ˆæ€æƒ³ï¼ˆWhyï¼‰\n",
        "\n",
        "```md\n",
        "## Training Objective\n",
        "\n",
        "This adapter is trained to improve **structured output accuracy**\n",
        "(JSON / YAML / XML / TOML / CSV).\n",
        "\n",
        "Loss is applied only to the final assistant output,\n",
        "while intermediate reasoning (Chain-of-Thought) is masked.\n",
        "```\n",
        "\n",
        "**ä»Šå›ã®è¬›åº§ã§ã¯ç‰¹ã«é‡è¦**\n",
        "\n",
        "* assistant-only loss\n",
        "* CoT maskï¼ˆOutput: ä»¥é™ã®ã¿å­¦ç¿’ï¼‰\n",
        "\n",
        "---\n",
        "\n",
        "##### 4. å­¦ç¿’è¨­å®šï¼ˆHowï¼‰\n",
        "\n",
        "```md\n",
        "## Training Configuration\n",
        "\n",
        "- Base model: Qwen3-4B-Instruct-2507\n",
        "- Method: QLoRA (4-bit)\n",
        "- Max sequence length: 512\n",
        "- Epochs: 1\n",
        "- Learning rate: 1e-6\n",
        "- LoRA: r=64, alpha=128\n",
        "```\n",
        "\n",
        "**å†ç¾æ€§ã®ãŸã‚å¿…é ˆ**\n",
        "\n",
        "---\n",
        "\n",
        "##### 5. ä½¿ç”¨æ–¹æ³•ï¼ˆHow to useï¼‰\n",
        "\n",
        "````md\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "adapter = \"your_id/your-repo\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "````\n",
        "\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "##### 6. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æ³¨æ„ï¼ˆå¿…é ˆãƒ»é‡è¦ï¼‰\n",
        "```md\n",
        "## Sources & Terms (IMPORTANT)\n",
        "\n",
        "Training data: u-10bei/structured_data_with_cot_dataset_512_v2\n",
        "\n",
        "Dataset License: MIT License. This dataset is used and distributed under the terms of the MIT License.\n",
        "Compliance: Users must comply with the MIT license (including copyright notice) and the base model's original terms of use.\n",
        "````\n",
        "---\n",
        "##### å®Ÿè¡Œã‚³ãƒ¼ãƒ‰ã®è¦‹æœ¬\n",
        "\n",
        "- ã€èª²é¡Œã€‘æœ€ä½é™ã€ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒˆãƒ«ã®æ¬„ã¯ã€å¿…ãšè‡ªèº«ã§æ›¸ãè¾¼ã‚“ã§ä¸‹ã•ã„ã€‚\n",
        "- ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å¤‰æ›´ã—ãŸå ´åˆã«ã¯ã€\"Dataset License\",\"Compliance\" æ¬„ã‚‚é©åˆ‡ãªå½¢ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚"
      ],
      "metadata": {
        "id": "t0AyjusDD2od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# README.mdï¼ˆãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ï¼‰ã‚’ OUT_LORA_DIR ã«ç”Ÿæˆ\n",
        "# -----------------------------\n",
        "# å­¦ç¿’å®Œäº†å¾Œã«å®Ÿè¡Œã—ã€Hugging Face ã® README.mdï¼ˆãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ï¼‰ã‚’ç”Ÿæˆ\n",
        "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«åãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåãƒ»å­¦ç¿’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚³ãƒ¼ãƒ‰ã®å¤‰æ•°ã‹ã‚‰è‡ªå‹•åŒæœŸ\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(OUT_LORA_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# è£œåŠ©é–¢æ•°ã®å®šç¾©\n",
        "# ------------------------------------------------------------------\n",
        "def _s(x, default=\"\"):\n",
        "    try:\n",
        "        v = str(x)\n",
        "        return v if v.strip() else default\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _fmt_lr(x) -> str:\n",
        "    \"\"\"\n",
        "    Learning Rate ã®è¡¨è¨˜ã‚’æ•´ãˆã‚‹ãŸã‚ã®é–¢æ•°ã€‚\n",
        "\n",
        "    - æ•°å€¤ã¨ã—ã¦è§£é‡ˆã§ãã‚‹å ´åˆï¼š\n",
        "      æŒ‡æ•°è¡¨è¨˜ï¼ˆä¾‹: 1e-6ï¼‰ã«æ•´å½¢ã™ã‚‹\n",
        "    - æ•°å€¤ã¨ã—ã¦è§£é‡ˆã§ããªã„å ´åˆï¼š\n",
        "      å…ƒã®å€¤ã‚’ãã®ã¾ã¾æ–‡å­—åˆ—ã¨ã—ã¦å‡ºåŠ›ã™ã‚‹\n",
        "      ï¼ˆèª¤ã£ãŸå€¤ã‚’ç”Ÿæˆã—ãªã„ãŸã‚ã®å®‰å…¨ç­–ï¼‰\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return f\"{float(x):.0e}\"\n",
        "    except Exception:\n",
        "        return _s(x, \"\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã®å¤‰æ•°ã‹ã‚‰å€¤ã‚’å–å¾—ï¼ˆREADME ã¨è‡ªå‹•åŒæœŸï¼‰\n",
        "# ------------------------------------------------------------------\n",
        "base_model_id = _s(BASE_MODEL_ID, \"Qwen/Qwen3-4B-Instruct-2507\")\n",
        "dataset_id = _s(DATASET_ID, \"https://huggingface.co/datasets/u-10bei/structured_data_with_cot_dataset_512_v2\")\n",
        "\n",
        "max_seq_len = int(MAX_SEQ_LEN)\n",
        "epochs = int(NUM_TRAIN_EPOCHS)\n",
        "lr_str = _fmt_lr(LR)\n",
        "\n",
        "lora_r = int(LORA_R)\n",
        "lora_alpha = int(LORA_ALPHA)\n",
        "\n",
        "# NOTE:\n",
        "# - YAML front matter ã® license ã¯\n",
        "#   ã€Œã“ã® LoRA ã‚¢ãƒ€ãƒ—ã‚¿ï¼ˆãƒªãƒã‚¸ãƒˆãƒªï¼‰ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹è¡¨æ˜ã€ã‚’æ„å‘³ã™ã‚‹ã€‚\n",
        "# - å¿…è¦ã«å¿œã˜ã¦ç’°å¢ƒå¤‰æ•°ã§å·®ã—æ›¿ãˆå¯èƒ½ã€‚\n",
        "repo_license = os.environ.get(\"SFT_REPO_LICENSE\", \"apache-2.0\")\n",
        "\n",
        "# README å†…ã«è¨˜è¼‰ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒˆãƒ«\n",
        "# å¤‰æ›´ã—ãŸã„å ´åˆã¯ README.md ã‚’æ‰‹æ›¸ãã§èª¿æ•´\n",
        "title_line = \"ï¼œã€èª²é¡Œã€‘ã“ã“ã¯è‡ªåˆ†ã§è¨˜å…¥ã—ã¦ä¸‹ã•ã„ï¼\" #ä¾‹ï¼š qwen3-4b-structured-output-lora\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# README.md æœ¬æ–‡ã®ç”Ÿæˆ\n",
        "# ï¼ˆèª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã«æº–æ‹ ã—ã€å¤‰æ•°éƒ¨åˆ†ã®ã¿ã‚’è‡ªå‹•ç½®æ›ï¼‰\n",
        "# ------------------------------------------------------------------\n",
        "readme_md = f\"\"\"---\n",
        "base_model: {base_model_id}\n",
        "datasets:\n",
        "- {dataset_id}\n",
        "language:\n",
        "- en\n",
        "license: {repo_license}\n",
        "library_name: peft\n",
        "pipeline_tag: text-generation\n",
        "tags:\n",
        "- qlora\n",
        "- lora\n",
        "- structured-output\n",
        "---\n",
        "\n",
        "{title_line}\n",
        "\n",
        "This repository provides a **LoRA adapter** fine-tuned from\n",
        "**{base_model_id}** using **QLoRA (4-bit, Unsloth)**.\n",
        "\n",
        "This repository contains **LoRA adapter weights only**.\n",
        "The base model must be loaded separately.\n",
        "\n",
        "## Training Objective\n",
        "\n",
        "This adapter is trained to improve **structured output accuracy**\n",
        "(JSON / YAML / XML / TOML / CSV).\n",
        "\n",
        "Loss is applied only to the final assistant output,\n",
        "while intermediate reasoning (Chain-of-Thought) is masked.\n",
        "\n",
        "## Training Configuration\n",
        "\n",
        "- Base model: {base_model_id}\n",
        "- Method: QLoRA (4-bit)\n",
        "- Max sequence length: {max_seq_len}\n",
        "- Epochs: {epochs}\n",
        "- Learning rate: {lr_str}\n",
        "- LoRA: r={lora_r}, alpha={lora_alpha}\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = \"{base_model_id}\"\n",
        "adapter = \"your_id/your-repo\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "```\n",
        "\n",
        "## Sources & Terms (IMPORTANT)\n",
        "\n",
        "Training data: {dataset_id}\n",
        "\n",
        "Dataset License: MIT License. This dataset is used and distributed under the terms of the MIT License.\n",
        "Compliance: Users must comply with the MIT license (including copyright notice) and the base model's original terms of use.\n",
        "\"\"\"\n",
        "# ------------------------------------------------------------------\n",
        "# README.md ã®æ›¸ãè¾¼ã¿\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "readme_path = os.path.join(OUT_LORA_DIR, \"README.md\")\n",
        "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(readme_md)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# å‹•ä½œç¢ºèª\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "assert os.path.exists(readme_path), \"README.md was not written.\"\n",
        "assert readme_md.lstrip().startswith(\"---\\n\"), (\n",
        "    \"README.md must start with YAML front matter.\"\n",
        ")\n",
        "# ä¿®æ­£: å…ˆé ­ã® --- ã¯æ”¹è¡Œãªã—ã§å§‹ã¾ã‚‹ãŸã‚ count(\"\\n---\\n\") ã«ã¯å«ã¾ã‚Œãªã„ã€‚\n",
        "# ãã®ãŸã‚ã€é–‰ã˜ã‚¿ã‚°ã®åˆ†ã¨ã—ã¦ 1å›ä»¥ä¸Šã‚ã‚Œã°OKã¨ã™ã‚‹ã€‚\n",
        "assert readme_md.count(\"\\n---\\n\") >= 1, (\n",
        "    \"YAML front matter must be closed properly.\"\n",
        ")\n",
        "\n",
        "print(f\"[INFO] README.md written to: {readme_path}\")\n",
        "print(\"[INFO] Preview (first 30 lines):\")\n",
        "for i, line in enumerate(readme_md.splitlines()[:30], start=1):\n",
        "    print(f\"{i:02d}: {line}\")"
      ],
      "metadata": {
        "id": "FhZ-2hZBSSSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### â‘¡ README.md ã® HF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰\n",
        "\n",
        "ä»¥ä¸‹ã¯ **README.md ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã›ã‚“**ã€‚\n",
        "\n",
        "* ç›´å‰ã®ã‚³ãƒ¼ãƒ‰ã‚’å‚ç…§ã—ã¦README.md å®Œæˆã•ã›ã€ OUT_LORA_DIR ã«ä¿å­˜ã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n",
        "* ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã¨ã—ã¦ README.md ã‚’å¿…é ˆåŒ–\n",
        "* README.md ãŒå­˜åœ¨ã—ãªã„å ´åˆ â†’ **ã‚¨ãƒ©ãƒ¼**\n",
        "---"
      ],
      "metadata": {
        "id": "1yne5up9W4OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3) LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’Hugging Faceã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ä½œæˆæ¸ˆã¿ã®READMEã‚’å«ã‚€)\n",
        "# ============================================================\n",
        "\n",
        "import fnmatch\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Hugging Face APIã®æ“ä½œç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
        "api = HfApi()\n",
        "\n",
        "# å„ç¨®ãƒ‘ã‚¹ã‚„è¨­å®šã®æº–å‚™\n",
        "LORA_SAVE_DIR = Path(OUT_LORA_DIR)  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "HF_REPO_ID    = _getenv(\"HF_REPO_ID\", \"your_id/your-lora-repo\")  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ¬ãƒã‚¸ãƒˆãƒªID\n",
        "\n",
        "# éå…¬é–‹è¨­å®šã®ç¢ºèªï¼ˆç’°å¢ƒå¤‰æ•°ãŒ '1' ã¾ãŸã¯ 'true' ãªã‚‰ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆè¨­å®šã«ã™ã‚‹ï¼‰\n",
        "PRIVATE       = _getenv(\"HF_PRIVATE\", \"1\") in (\"1\",\"true\",\"True\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3.1) å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
        "# -----------------------------\n",
        "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«æœ€ä½é™å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®šç¾©ã—ã¾ã™\n",
        "required_files = {\n",
        "    \"adapter_config.json\", # LoRAã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«\n",
        "    \"README.md\",           # å—è¬›ç”ŸãŒä½œæˆã—ãŸè§£èª¬æ–‡æ›¸\n",
        "}\n",
        "\n",
        "# ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
        "present = {p.name for p in LORA_SAVE_DIR.iterdir() if p.is_file()}\n",
        "\n",
        "# è¶³ã‚Šãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
        "missing = [f for f in required_files if f not in present]\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ï¼ˆadapter_model.safetensors ã¾ãŸã¯ .binï¼‰ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
        "if not any(f.startswith(\"adapter_model.\") for f in present):\n",
        "    missing.append(\"adapter_model.(safetensors|bin)\")\n",
        "\n",
        "# å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã—ã¦å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™\n",
        "if missing:\n",
        "    raise RuntimeError(\n",
        "        \"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚\\n\"\n",
        "        \"ä»¥ä¸‹ã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\\n\"\n",
        "        + \"\\n\".join(f\"- {m}\" for m in missing) +\n",
        "        \"\\n\\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‰ã«ã€README.md ã‚’æ‰‹æ›¸ãã§ä½œæˆã—ä¿å­˜ã—ã¦ãã ã•ã„ã€‚\"\n",
        "    )\n",
        "\n",
        "print(\"âœ… å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3.2) ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®é¸åˆ¥ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼‰\n",
        "# -----------------------------\n",
        "# ä¸è¦ãªä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãªã„ã‚ˆã†ã€è¨±å¯ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æŒ‡å®šã—ã¾ã™\n",
        "ALLOW_PATTERNS = [\n",
        "    \"README.md\",\n",
        "    \"adapter_config.json\",\n",
        "    \"adapter_model.*\",\n",
        "    \"tokenizer.*\",\n",
        "    \"special_tokens_map.json\",\n",
        "    \"*.json\",\n",
        "]\n",
        "\n",
        "def is_allowed(name: str) -> bool:\n",
        "    \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«åãŒè¨±å¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ã‹åˆ¤å®šã™ã‚‹é–¢æ•°\"\"\"\n",
        "    return any(fnmatch.fnmatch(name, pat) for pat in ALLOW_PATTERNS)\n",
        "\n",
        "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°é ˜åŸŸï¼‰ã‚’ä½œæˆ\n",
        "STAGE_DIR = Path(\"/content/hf_upload_stage\")\n",
        "\n",
        "if STAGE_DIR.exists():\n",
        "    shutil.rmtree(STAGE_DIR) # æ—¢å­˜ã®ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Œã°ä¸€æ—¦å‰Šé™¤\n",
        "STAGE_DIR.mkdir(parents=True)\n",
        "\n",
        "# è¨±å¯ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚’ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚³ãƒ”ãƒ¼\n",
        "for p in LORA_SAVE_DIR.iterdir():\n",
        "    if p.is_file() and is_allowed(p.name):\n",
        "        (STAGE_DIR / p.name).write_bytes(p.read_bytes())\n",
        "\n",
        "print(\"ğŸ“¦ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«:\", [p.name for p in STAGE_DIR.iterdir()])\n",
        "\n",
        "# -----------------------------\n",
        "# 3.3) ãƒªãƒã‚¸ãƒˆãƒªä½œæˆã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "# -----------------------------\n",
        "\n",
        "# Hugging Faceä¸Šã«ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã—ã¦ã„ã¦ã‚‚OKï¼‰\n",
        "api.create_repo(\n",
        "    repo_id=HF_REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    private=PRIVATE,\n",
        ")\n",
        "\n",
        "# ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã®å†…å®¹ã‚’ã¾ã‚‹ã”ã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "api.upload_folder(\n",
        "    folder_path=str(STAGE_DIR),\n",
        "    repo_id=HF_REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload LoRA adapter (README written by author)\",\n",
        ")\n",
        "\n",
        "print(\"âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "print(f\"URL: https://huggingface.co/{HF_REPO_ID}\")"
      ],
      "metadata": {
        "id": "7bLNMxfZOwIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}