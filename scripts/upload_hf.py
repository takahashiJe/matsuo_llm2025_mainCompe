#!/usr/bin/env python3

# python scripts/upload_hf.py --merged --model-dir outputs/models/20260206-005340_merged
# python scripts/upload_hf.py --run-id 20260206-005340 --repo-id JuntaTakahashi/qwen3-4b-structured-sft-lora-adapter
# python scripts/upload_hf.py --run-id 20260212_v1/phase3 --repo-id JuntaTakahashi/qwen3-4b-structured-dpo-lora

import argparse
import fnmatch
import json
import os
import shutil
from pathlib import Path

from huggingface_hub import HfApi


def _load_dotenv(path: Path) -> None:
    if not path.exists():
        return
    for raw in path.read_text().splitlines():
        line = raw.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        value = value.strip().strip("'").strip('"')
        if key and key not in os.environ:
            os.environ[key] = value


def _getenv(key: str, default: str | None = None) -> str:
    value = os.environ.get(key, default)
    if value is None or value == "":
        raise RuntimeError(f"ç’°å¢ƒå¤‰æ•° {key} ãŒæœªè¨­å®šã§ã™ã€‚")
    return value


def _resolve_model_dir(run_id: str | None, model_dir: str | None) -> Path:
    if model_dir:
        return Path(model_dir)
    if run_id:
        return Path("outputs") / "models" / run_id
    raise RuntimeError("--run-id ã‹ --model-dir ã®ã©ã¡ã‚‰ã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")


def main() -> None:
    parser = argparse.ArgumentParser(description="Upload model/adapter to Hugging Face Hub")
    parser.add_argument("--run-id", help="outputs/models/{run_id} ã® run_id")
    parser.add_argument("--model-dir", help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument(
        "--merged",
        action="store_true",
        help="ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹",
    )
    parser.add_argument(
        "--stage-dir",
        default="outputs/hf_upload_stage",
        help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: outputs/hf_upload_stage)",
    )
    parser.add_argument(
        "--repo-id",
        help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆã® Hugging Face ãƒªãƒã‚¸ãƒˆãƒª (å„ªå…ˆ)ã€‚ä¾‹: JuntaTakahashi/qwen3-4b-structured-sft-lora",
    )
    parser.add_argument(
        "--repo-env",
        default="HF_LORA_REPO",
        help="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆãƒªãƒã‚¸ãƒˆãƒªã‚’å–å¾—ã™ã‚‹ç’°å¢ƒå¤‰æ•°å (default: HF_REPO)",
    )
    args = parser.parse_args()

    _load_dotenv(Path(".env"))

    lora_save_dir = _resolve_model_dir(args.run_id, args.model_dir)
    if not lora_save_dir.exists():
        raise RuntimeError(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {lora_save_dir}")

    hf_token = _getenv("HF_API")
    hf_repo_id = args.repo_id or _getenv(args.repo_env)
    private = _getenv("HF_PRIVATE", "1") in ("1", "true", "True")

    api = HfApi(token=hf_token)

    # 3.1) å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    present = {p.name for p in lora_save_dir.iterdir() if p.is_file()}
    if args.merged:
        # merged ãƒ¢ãƒ‡ãƒ«ã« adapter_config.json ãŒã‚ã‚‹ã®ã¯ç•°å¸¸
        if "adapter_config.json" in present:
            raise RuntimeError(
                "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚\n"
                "merged ãƒ¢ãƒ‡ãƒ«ã« adapter_config.json ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n"
                "ã“ã‚Œã¯ LoRA ã‚¢ãƒ€ãƒ—ã‚¿ã®æ§‹æˆã§ã™ã€‚merged ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
            )
        required_files = {"config.json", "README.md"}
        missing = [f for f in required_files if f not in present]
        if not (
            any(fnmatch.fnmatch(name, "model.*") for name in present)
            or any(fnmatch.fnmatch(name, "pytorch_model.*") for name in present)
        ):
            missing.append("model.(safetensors|bin) or pytorch_model.(bin|safetensors)")
        if "config.json" in present:
            try:
                cfg = json.loads((lora_save_dir / "config.json").read_text(encoding="utf-8"))
                # PEFT/adapterç³»ã®ç—•è·¡ãŒã‚ã‚Œã°mergedã¨ã—ã¦ã¯ä¸æ­£
                if any(k in cfg for k in ("peft_type", "adapter_config", "adapter_type")):
                    raise RuntimeError(
                        "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚\n"
                        "config.json ã« PEFT/adapter ã®è¨­å®šãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n"
                        "ã“ã‚Œã¯ merged ãƒ¢ãƒ‡ãƒ«ã§ã¯ãªãã‚¢ãƒ€ãƒ—ã‚¿æ§‹æˆã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                    )
            except json.JSONDecodeError as e:
                raise RuntimeError(f"config.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}") from e
    else:
        required_files = {"adapter_config.json", "README.md"}
        missing = [f for f in required_files if f not in present]
        if not any(name.startswith("adapter_model.") for name in present):
            missing.append("adapter_model.(safetensors|bin)")
    if missing:
        raise RuntimeError(
            "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚\n"
            "ä»¥ä¸‹ã®å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\n"
            + "\n".join(f"- {m}" for m in missing)
            + "\n\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‰ã«ã€README.md ã‚’æ‰‹æ›¸ãã§ä½œæˆã—ä¿å­˜ã—ã¦ãã ã•ã„ã€‚"
        )

    print("âœ… å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # 3.2) ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®é¸åˆ¥ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼‰
    if args.merged:
        allow_patterns = [
            "README.md",
            "config.json",
            "model.*",
            "model-*",
            "model-*.safetensors",
            "model-*.bin",
            "pytorch_model.*",
            "pytorch_model-*",
            "pytorch_model-*.bin",
            "pytorch_model-*.safetensors",
            "chat_template.jinja",
            "tokenizer.*",
            "special_tokens_map.json",
            "*.json",
        ]
    else:
        allow_patterns = [
            "README.md",
            "adapter_config.json",
            "adapter_model.*",
            "tokenizer.*",
            "special_tokens_map.json",
            "*.json",
        ]

    def is_allowed(name: str) -> bool:
        return any(fnmatch.fnmatch(name, pat) for pat in allow_patterns)

    stage_dir = Path(args.stage_dir)
    if stage_dir.exists():
        shutil.rmtree(stage_dir)
    stage_dir.mkdir(parents=True)

    for p in lora_save_dir.iterdir():
        if p.is_file() and is_allowed(p.name):
            if args.merged and p.name == "tokenizer_config.json":
                data = json.loads(p.read_text(encoding="utf-8"))
                if not data.get("chat_template"):
                    tmpl = lora_save_dir / "chat_template.jinja"
                    if tmpl.exists():
                        data["chat_template"] = tmpl.read_text(encoding="utf-8")
                (stage_dir / p.name).write_text(
                    json.dumps(data, ensure_ascii=False, indent=2),
                    encoding="utf-8",
                )
            else:
                (stage_dir / p.name).write_bytes(p.read_bytes())

    print("ğŸ“¦ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«:", [p.name for p in stage_dir.iterdir()])

    # 3.3) ãƒªãƒã‚¸ãƒˆãƒªä½œæˆã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    api.create_repo(
        repo_id=hf_repo_id,
        repo_type="model",
        exist_ok=True,
        private=private,
    )

    api.upload_folder(
        folder_path=str(stage_dir),
        repo_id=hf_repo_id,
        repo_type="model",
        commit_message="Upload LoRA adapter (README written by author)",
    )

    print("âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
    print(f"URL: https://huggingface.co/{hf_repo_id}")


if __name__ == "__main__":
    main()
