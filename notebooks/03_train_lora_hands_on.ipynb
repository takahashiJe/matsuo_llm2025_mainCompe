{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: TRL + Unsloth QLoRA training\n",
    "\n",
    "このノートブックは `scripts/train_lora.py` を**ブロック単位で理解しながら実行**できるように分解したものです。\n",
    "重い処理があるので、各セルの説明を読んでから実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ace3051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/junta_takahashi/matsuo_llm2025_mainCompe/.maLM25main/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffd5535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5e0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 依存の読み込みimport datetime as dt\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "from transformers import EarlyStoppingCallback, TrainingArguments, TrainerCallback\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "MASK_TAGS = [\"Output:\", \"OUTPUT:\", \"Final:\", \"Answer:\", \"Result:\", \"Response:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04d53a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/junta_takahashi/matsuo_llm2025_mainCompe\n"
     ]
    }
   ],
   "source": [
    "# 0.5 プロジェクトルートを解決（notebooks配下から実行する前提）\n",
    "# 目的: notebooks から実行しても正しいパスを参照できるようにする\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "# configs/ が存在する階層を上に探す\n",
    "PROJECT_ROOT = next(p for p in [cwd] + list(cwd.parents) if (p / \"configs\" / \"train_lora.yaml\").exists())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a01fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model: Qwen/Qwen3-4B-Instruct-2507\n",
      "data_path: /home/junta_takahashi/matsuo_llm2025_mainCompe/data/processed/merged_train.jsonl\n",
      "output_dir: /home/junta_takahashi/matsuo_llm2025_mainCompe/outputs/models/20260205-011456\n"
     ]
    }
   ],
   "source": [
    "# 1. 設定読み込み\n",
    "import datetime as dt\n",
    "# 目的: configs/train_lora.yaml から設定を読み、以降のセルで使います。\n",
    "# 設定の内容:\n",
    "# - data: データパス・分割比率・乱数seed\n",
    "# - training: 学習ハイパーパラメータ（batch/epoch/max_length 等）\n",
    "# - lora: LoRAのrank/alpha/対象層\n",
    "# - model: ベースモデル名\n",
    "# ここで run_name / 出力ディレクトリ が決まります。\n",
    "cfg = yaml.safe_load((PROJECT_ROOT / \"configs\" / \"train_lora.yaml\").read_text(encoding=\"utf-8\"))\n",
    "data_cfg = cfg[\"data\"]\n",
    "train_cfg = cfg[\"training\"]\n",
    "lora_cfg = cfg[\"lora\"]\n",
    "model_cfg = cfg[\"model\"]\n",
    "\n",
    "data_path = data_cfg[\"data_path\"]\n",
    "data_path = str((PROJECT_ROOT / data_path).resolve())\n",
    "eval_ratio = float(data_cfg[\"eval_ratio\"])\n",
    "seed = int(data_cfg[\"seed\"])\n",
    "base_model = model_cfg[\"base_model\"]\n",
    "max_length = int(train_cfg[\"max_length\"])\n",
    "\n",
    "output_dir = str(PROJECT_ROOT / train_cfg[\"output_dir\"] / dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "run_name = Path(output_dir).name\n",
    "\n",
    "print(\"base_model:\", base_model)\n",
    "print(\"data_path:\", data_path)\n",
    "print(\"output_dir:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2162e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9e5973999947fda61f6e8ffbc58237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/builder.py:1834\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1833\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1834\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1835\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/arrow_writer.py:714\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    713\u001b[39m pa_table = pa_table.combine_chunks()\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m pa_table = \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed_local_files:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/table.py:2272\u001b[39m, in \u001b[36mtable_cast\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table.schema != schema:\n\u001b[32m-> \u001b[39m\u001b[32m2272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table.schema.metadata != schema.metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/table.py:2223\u001b[39m, in \u001b[36mcast_table_to_schema\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[32m   2219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table.schema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbecause column names don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2220\u001b[39m         table_column_names=table.column_names,\n\u001b[32m   2221\u001b[39m         requested_column_names=\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[32m   2222\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2223\u001b[39m arrays = \u001b[43m[\u001b[49m\n\u001b[32m   2224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable_column_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2228\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2229\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_arrays(arrays, schema=schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/table.py:2224\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[32m   2219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table.schema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbecause column names don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2220\u001b[39m         table_column_names=table.column_names,\n\u001b[32m   2221\u001b[39m         requested_column_names=\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[32m   2222\u001b[39m     )\n\u001b[32m   2223\u001b[39m arrays = [\n\u001b[32m-> \u001b[39m\u001b[32m2224\u001b[39m     \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable_column_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2228\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m   2229\u001b[39m ]\n\u001b[32m   2230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_arrays(arrays, schema=schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/table.py:1795\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa.ChunkedArray):\n\u001b[32m-> \u001b[39m\u001b[32m1795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array(\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/table.py:1795\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa.ChunkedArray):\n\u001b[32m-> \u001b[39m\u001b[32m1795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/table.py:2092\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2086\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array_cast(\n\u001b[32m   2087\u001b[39m         array,\n\u001b[32m   2088\u001b[39m         feature(),\n\u001b[32m   2089\u001b[39m         allow_primitive_to_str=allow_primitive_to_str,\n\u001b[32m   2090\u001b[39m         allow_decimal_to_str=allow_decimal_to_str,\n\u001b[32m   2091\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2092\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(array.type)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(feature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Couldn't cast array of type\nstruct<format: string, complexity: string, schema: string, constraint: string, type: string, prompt: string, output: string, estimated_tokens: int64, source: string, id: string, category: string, subcategory: string, task: string, seed: string>\nto\n{'format': Value('string'), 'complexity': Value('string'), 'schema': Value('string'), 'constraint': Value('string'), 'type': Value('string'), 'prompt': Value('string'), 'output': Value('string'), 'estimated_tokens': Value('int64'), 'source': Value('string')}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. データ読み込み + 分割\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 目的: merged_train.jsonl を読み込み、train/eval に分割します。\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 注意: この時点では messages を保持しておき、後で生成ログに使います。\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 出力: ds_raw (train/test) が作成されます。\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ここでは元データ (messages付き) を保持します。\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m ds_raw = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m ds_raw = ds_raw.train_test_split(test_size=eval_ratio, seed=seed)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrain size:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ds_raw[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/load.py:1417\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   1416\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1426\u001b[39m keep_in_memory = (\n\u001b[32m   1427\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1428\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/builder.py:897\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    896\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/builder.py:973\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    972\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    976\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    977\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    978\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    979\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m    980\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/builder.py:1705\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1703\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/matsuo_llm2025_mainCompe/.maLM25main/lib/python3.11/site-packages/datasets/builder.py:1861\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1859\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[32m   1860\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1861\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1863\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "# 2. データ読み込み + 分割\n",
    "# 目的: merged_train.jsonl を読み込み、train/eval に分割します。\n",
    "# 注意: この時点では messages を保持しておき、後で生成ログに使います。\n",
    "# 出力: ds_raw (train/test) が作成されます。\n",
    "# ここでは元データ (messages付き) を保持します。\n",
    "ds_raw = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "ds_raw = ds_raw.train_test_split(test_size=eval_ratio, seed=seed)\n",
    "print(\"train size:\", len(ds_raw[\"train\"]))\n",
    "print(\"eval size:\", len(ds_raw[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b66203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. モデル・トークナイザ初期化（Unsloth）\n",
    "# 目的: Unslothで4bit量子化のモデルを読み込みます。\n",
    "# ここでGPUに載るため、VRAM消費が大きいセルです。\n",
    "# max_length が長いとVRAM使用量が増える点に注意。\n",
    "# VRAMに厳しい場合は max_length を下げてください。\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model,\n",
    "    max_seq_length=max_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"pad_token:\", tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. メッセージをテキストに変換する関数\n",
    "# 目的: messages -> ChatML風テキストに変換する関数を定義します。\n",
    "# - messages_to_text: 学習用（全文）\n",
    "# - build_prompt_from_messages: 生成用（assistant直前まで）\n",
    "# - extract_assistant_ref: 参照回答を取り出す\n",
    "# 学習用 (全文) と生成用 (assistant直前まで) を分けています。\n",
    "def messages_to_text(messages: List[dict]) -> str:\n",
    "    parts = []\n",
    "    for m in messages:\n",
    "        if not isinstance(m, dict):\n",
    "            continue\n",
    "        role = m.get(\"role\", \"user\")\n",
    "        content = m.get(\"content\", \"\")\n",
    "        parts.append(f\"<|{role}|>\\n{content}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def build_prompt_from_messages(messages: List[dict]) -> str:\n",
    "    parts = []\n",
    "    for m in messages:\n",
    "        if not isinstance(m, dict):\n",
    "            continue\n",
    "        role = m.get(\"role\", \"user\")\n",
    "        content = m.get(\"content\", \"\")\n",
    "        if role == \"assistant\":\n",
    "            break\n",
    "        parts.append(f\"<|{role}|>\\n{content}\")\n",
    "    parts.append(\"<|assistant|>\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def extract_assistant_ref(messages: List[dict]) -> str:\n",
    "    for m in messages:\n",
    "        if isinstance(m, dict) and m.get(\"role\") == \"assistant\":\n",
    "            return str(m.get(\"content\", \"\"))\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. テキスト化 + カラム整形\n",
    "# 目的: 学習用の text カラムだけを作り、Trainerに渡せる形にします。\n",
    "# これ以降の学習は text カラムのみを使います。\n",
    "# ここから先は text を使って学習します。\n",
    "def format_fn(example: Dict[str, object]) -> Dict[str, str]:\n",
    "    messages = example.get(\"messages\", [])\n",
    "    return {\"text\": messages_to_text(messages)}\n",
    "\n",
    "ds = ds_raw.map(format_fn, remove_columns=ds_raw[\"train\"].column_names)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Outputタグ出現率\n",
    "# 目的: Output系タグがどの程度含まれているか確認します。\n",
    "# ここで値が低すぎると loss マスクが効きにくい可能性があります。\n",
    "def has_tag(text: str) -> bool:\n",
    "    return any(tag in text for tag in MASK_TAGS)\n",
    "\n",
    "tag_hits = sum(1 for t in ds[\"train\"][\"text\"] if has_tag(t))\n",
    "print(f\"Tag hit ratio (train): {tag_hits}/{len(ds['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf30c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. トークン長分布の保存\n",
    "# 目的: token長分布を保存し、max_lengthの妥当性を確認します。\n",
    "# 併せて run_meta.yaml に学習条件を記録します。\n",
    "# run_meta.yaml / length_stats.yaml が outputs に保存されます。\n",
    "def percentile(values: List[int], p: float) -> int:\n",
    "    if not values:\n",
    "        return 0\n",
    "    idx = int(round((p / 100.0) * (len(values) - 1)))\n",
    "    return values[idx]\n",
    "\n",
    "def compute_length_stats(texts: List[str], tokenizer, max_length: int) -> Dict[str, int]:\n",
    "    lengths: List[int] = []\n",
    "    for t in texts:\n",
    "        ids = tokenizer(t, add_special_tokens=False, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        lengths.append(len(ids))\n",
    "    lengths.sort()\n",
    "    return {\n",
    "        \"samples\": len(lengths),\n",
    "        \"p50\": percentile(lengths, 50),\n",
    "        \"p75\": percentile(lengths, 75),\n",
    "        \"p90\": percentile(lengths, 90),\n",
    "        \"p95\": percentile(lengths, 95),\n",
    "        \"p97\": percentile(lengths, 97),\n",
    "        \"p98\": percentile(lengths, 98),\n",
    "        \"p99\": percentile(lengths, 99),\n",
    "        \"p99_5\": percentile(lengths, 99.5),\n",
    "        \"max\": lengths[-1] if lengths else 0,\n",
    "    }\n",
    "\n",
    "def save_yaml(path: Path, payload: Dict[str, object]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(yaml.safe_dump(payload, allow_unicode=True), encoding=\"utf-8\")\n",
    "\n",
    "run_meta = {\n",
    "    \"base_model\": base_model,\n",
    "    \"data_path\": data_path,\n",
    "    \"eval_ratio\": eval_ratio,\n",
    "    \"seed\": seed,\n",
    "    \"max_length\": max_length,\n",
    "    \"batch_size\": int(train_cfg[\"batch_size\"]),\n",
    "    \"grad_accum\": int(train_cfg[\"grad_accum\"]),\n",
    "    \"learning_rate\": float(train_cfg[\"learning_rate\"]),\n",
    "    \"epochs\": float(train_cfg[\"epochs\"]),\n",
    "    \"precision\": str(train_cfg[\"precision\"]),\n",
    "    \"packing\": bool(train_cfg[\"packing\"]),\n",
    "    \"lora_r\": int(lora_cfg[\"r\"]),\n",
    "    \"lora_alpha\": int(lora_cfg[\"alpha\"]),\n",
    "    \"lora_target_modules\": list(lora_cfg[\"target_modules\"]),\n",
    "    \"quantization\": {\n",
    "        \"load_in_4bit\": True,\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",\n",
    "        \"bnb_4bit_use_double_quant\": True,\n",
    "        \"bnb_4bit_compute_dtype\": \"bf16\",\n",
    "    },\n",
    "    \"mask_tags\": MASK_TAGS,\n",
    "    \"tag_hit_train\": tag_hits,\n",
    "    \"train_size\": len(ds[\"train\"]),\n",
    "    \"eval_size\": len(ds[\"test\"]),\n",
    "}\n",
    "\n",
    "length_stats = compute_length_stats(ds[\"train\"][\"text\"], tokenizer, max_length=max_length)\n",
    "save_yaml(Path(output_dir) / \"run_meta.yaml\", run_meta)\n",
    "save_yaml(Path(output_dir) / \"length_stats.yaml\", length_stats)\n",
    "\n",
    "print(\"Saved run_meta.yaml and length_stats.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. LoRA 設定 + gradient checkpointing（Unsloth）\n",
    "# 目的: LoRAを有効化し、Unslothのgradient checkpointingでVRAM節約します。\n",
    "# ここで LoRA のランク(r)やalphaが反映されます。\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=int(lora_cfg[\"r\"]),\n",
    "    lora_alpha=int(lora_cfg[\"alpha\"]),\n",
    "    target_modules=list(lora_cfg[\"target_modules\"]),\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=seed,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "print(\"LoRA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfe4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. labels を Outputタグ以降に限定する collator\n",
    "# 目的: Outputタグ以降のみ loss を計算するためのcollatorを作ります。\n",
    "# - タグが無い場合は全文をloss対象にします。\n",
    "def find_first_tag_pos(text: str, tags: List[str]) -> Optional[Tuple[int, str]]:\n",
    "    found = []\n",
    "    for tag in tags:\n",
    "        idx = text.find(tag)\n",
    "        if idx >= 0:\n",
    "            found.append((idx, tag))\n",
    "    if not found:\n",
    "        return None\n",
    "    found.sort(key=lambda x: x[0])\n",
    "    return found[0]\n",
    "\n",
    "@dataclass\n",
    "class CollatorConfig:\n",
    "    tokenizer: object\n",
    "    max_length: int\n",
    "    tags: List[str]\n",
    "\n",
    "class OutputTagCollator:\n",
    "    def __init__(self, cfg: CollatorConfig) -> None:\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, object]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = [f[\"text\"] for f in features]\n",
    "        tok = self.cfg.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.cfg.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = tok[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "        for i, text in enumerate(texts):\n",
    "            hit = find_first_tag_pos(text, self.cfg.tags)\n",
    "            if hit is None:\n",
    "                continue\n",
    "            tag_pos, tag = hit\n",
    "            prefix = text[: tag_pos + len(tag)]\n",
    "            prefix_ids = self.cfg.tokenizer(\n",
    "                prefix, truncation=True, max_length=self.cfg.max_length, add_special_tokens=False\n",
    "            )[\"input_ids\"]\n",
    "            cut = len(prefix_ids)\n",
    "            labels[i, :cut] = -100\n",
    "        tok[\"labels\"] = labels\n",
    "        return tok\n",
    "\n",
    "collator = OutputTagCollator(\n",
    "    CollatorConfig(tokenizer=tokenizer, max_length=max_length, tags=MASK_TAGS)\n",
    ")\n",
    "print(\"collator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6703519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. エポック終了時のサンプル生成ログ\n",
    "# 目的: 各エポック終了時に固定サンプルを生成し、ログを保存します。\n",
    "# - 人間が見て改善点を判断できる出力を残します。\n",
    "# 人間が見て改善点を判断できるログになります。\n",
    "class EvalSampleCallback(TrainerCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        output_dir: str,\n",
    "        sample_size: int,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float,\n",
    "        top_p: float,\n",
    "        seed: int,\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.sample_size = sample_size\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        rng = torch.Generator().manual_seed(seed)\n",
    "        n = len(dataset)\n",
    "        if n == 0:\n",
    "            self.indices = []\n",
    "        else:\n",
    "            perm = torch.randperm(n, generator=rng).tolist()\n",
    "            self.indices = perm[: min(sample_size, n)]\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        model_was_training = model.training\n",
    "        model.eval()\n",
    "\n",
    "        results = []\n",
    "        device = next(model.parameters()).device\n",
    "        for idx in self.indices:\n",
    "            ex = self.dataset[idx]\n",
    "            messages = ex.get(\"messages\", [])\n",
    "            prompt = build_prompt_from_messages(messages)\n",
    "            ref = extract_assistant_ref(messages)\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, return_tensors=\"pt\", add_special_tokens=False\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=self.temperature,\n",
    "                    top_p=self.top_p,\n",
    "                )\n",
    "            text = self.tokenizer.decode(out[0], skip_special_tokens=False)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"epoch\": float(state.epoch),\n",
    "                    \"index\": idx,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"prediction\": text,\n",
    "                    \"reference\": ref,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        out_path = self.output_dir / f\"eval_samples_epoch{int(state.epoch)}.jsonl\"\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for r in results:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if model_was_training:\n",
    "            model.train()\n",
    "\n",
    "print(\"EvalSampleCallback ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17eb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. TrainingArguments\n",
    "# 目的: 学習ループの詳細設定を行います。\n",
    "# - eval_steps / save_steps / precision など\n",
    "# - report_to=\"wandb\" でW&Bにログ送信\n",
    "# WandB の run_name もここで渡します。\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=float(train_cfg[\"epochs\"]),\n",
    "    per_device_train_batch_size=int(train_cfg[\"batch_size\"]),\n",
    "    per_device_eval_batch_size=int(train_cfg[\"batch_size\"]),\n",
    "    gradient_accumulation_steps=int(train_cfg[\"grad_accum\"]),\n",
    "    learning_rate=float(train_cfg[\"learning_rate\"]),\n",
    "    save_steps=int(train_cfg[\"save_steps\"]),\n",
    "    eval_steps=int(train_cfg[\"eval_steps\"]),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    bf16=str(train_cfg[\"precision\"]).lower() == \"bf16\",\n",
    "    fp16=str(train_cfg[\"precision\"]).lower() == \"fp16\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "print(\"TrainingArguments ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Trainer 作成\n",
    "# 目的: 本番学習用のTrainerを作成します。\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    formatting_func=lambda x: x[\"text\"],\n",
    "    data_collator=collator,\n",
    "    args=args,\n",
    "    packing=bool(train_cfg[\"packing\"]),\n",
    "    max_seq_length=max_length,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer.add_callback(\n",
    "    EarlyStoppingCallback(\n",
    "        early_stopping_patience=int(train_cfg[\"early_stopping_patience\"]),\n",
    "        early_stopping_threshold=float(train_cfg[\"early_stopping_threshold\"]),\n",
    "    )\n",
    ")\n",
    "trainer.add_callback(\n",
    "    EvalSampleCallback(\n",
    "        dataset=ds_raw[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=output_dir,\n",
    "        sample_size=int(train_cfg[\"eval_sample_size\"]),\n",
    "        max_new_tokens=int(train_cfg[\"eval_max_new_tokens\"]),\n",
    "        temperature=float(train_cfg[\"eval_temperature\"]),\n",
    "        top_p=float(train_cfg[\"eval_top_p\"]),\n",
    "        seed=seed,\n",
    "    )\n",
    ")\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d02863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. 学習実行\n",
    "# 目的: 本番学習を実行します（時間がかかります）。\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. 保存\n",
    "# 目的: LoRAアダプタとtokenizerを保存します。\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Saved model to:\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".maLM25main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
