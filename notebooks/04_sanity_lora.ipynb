{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: TRL + Unsloth QLoRA training\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ `scripts/train_lora.py` ã‚’**ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§ç†è§£ã—ãªãŒã‚‰å®Ÿè¡Œ**ã§ãã‚‹ã‚ˆã†ã«åˆ†è§£ã—ãŸã‚‚ã®ã§ã™ã€‚\n",
    "é‡ã„å‡¦ç†ãŒã‚ã‚‹ã®ã§ã€å„ã‚»ãƒ«ã®èª¬æ˜ã‚’èª­ã‚“ã§ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ace3051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/junta_takahashi/matsuo_llm2025_mainCompe/.maLM25main/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffd5535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5e0ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4012150/3095100776.py:12: UserWarning: WARNING: Unsloth should be imported before [trl, transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# 0. ä¾å­˜ã®èª­ã¿è¾¼ã¿import datetime as dt\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "from transformers import EarlyStoppingCallback, TrainingArguments, TrainerCallback\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "MASK_TAGS = [\"Output:\", \"OUTPUT:\", \"Final:\", \"Answer:\", \"Result:\", \"Response:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d53a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è§£æ±ºï¼ˆnotebooksé…ä¸‹ã‹ã‚‰å®Ÿè¡Œã™ã‚‹å‰æï¼‰\n",
    "# ç›®çš„: notebooks ã‹ã‚‰å®Ÿè¡Œã—ã¦ã‚‚æ­£ã—ã„ãƒ‘ã‚¹ã‚’å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "# configs/ ãŒå­˜åœ¨ã™ã‚‹éšå±¤ã‚’ä¸Šã«æ¢ã™\n",
    "PROJECT_ROOT = next(p for p in [cwd] + list(cwd.parents) if (p / \"configs\" / \"train_lora.yaml\").exists())\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59a01fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model: Qwen/Qwen3-4B-Instruct-2507\n",
      "data_path: /home/junta_takahashi/matsuo_llm2025_mainCompe/data/processed/merged_train.jsonl\n",
      "output_dir: /home/junta_takahashi/matsuo_llm2025_mainCompe/outputs/models/20260205-005231\n"
     ]
    }
   ],
   "source": [
    "# 1. è¨­å®šèª­ã¿è¾¼ã¿\n",
    "import datetime as dt\n",
    "# ç›®çš„: configs/train_lora.yaml ã‹ã‚‰è¨­å®šã‚’èª­ã¿ã€ä»¥é™ã®ã‚»ãƒ«ã§ä½¿ã„ã¾ã™ã€‚\n",
    "# è¨­å®šã®å†…å®¹:\n",
    "# - data: ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ãƒ»åˆ†å‰²æ¯”ç‡ãƒ»ä¹±æ•°seed\n",
    "# - training: å­¦ç¿’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆbatch/epoch/max_length ç­‰ï¼‰\n",
    "# - lora: LoRAã®rank/alpha/å¯¾è±¡å±¤\n",
    "# - model: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å\n",
    "# ã“ã“ã§ run_name / å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ãŒæ±ºã¾ã‚Šã¾ã™ã€‚\n",
    "cfg = yaml.safe_load((PROJECT_ROOT / \"configs\" / \"train_lora.yaml\").read_text(encoding=\"utf-8\"))\n",
    "data_cfg = cfg[\"data\"]\n",
    "train_cfg = cfg[\"training\"]\n",
    "lora_cfg = cfg[\"lora\"]\n",
    "model_cfg = cfg[\"model\"]\n",
    "\n",
    "data_path = data_cfg[\"data_path\"]\n",
    "data_path = str((PROJECT_ROOT / data_path).resolve())\n",
    "eval_ratio = float(data_cfg[\"eval_ratio\"])\n",
    "seed = int(data_cfg[\"seed\"])\n",
    "base_model = model_cfg[\"base_model\"]\n",
    "max_length = int(train_cfg[\"max_length\"])\n",
    "\n",
    "output_dir = str(PROJECT_ROOT / train_cfg[\"output_dir\"] / dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "run_name = Path(output_dir).name\n",
    "\n",
    "print(\"base_model:\", base_model)\n",
    "print(\"data_path:\", data_path)\n",
    "print(\"output_dir:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2162e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ + åˆ†å‰²\n",
    "# ç›®çš„: merged_train.jsonl ã‚’èª­ã¿è¾¼ã¿ã€train/eval ã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
    "# æ³¨æ„: ã“ã®æ™‚ç‚¹ã§ã¯ messages ã‚’ä¿æŒã—ã¦ãŠãã€å¾Œã§ç”Ÿæˆãƒ­ã‚°ã«ä½¿ã„ã¾ã™ã€‚\n",
    "# å‡ºåŠ›: ds_raw (train/test) ãŒä½œæˆã•ã‚Œã¾ã™ã€‚\n",
    "# ã“ã“ã§ã¯å…ƒãƒ‡ãƒ¼ã‚¿ (messagesä»˜ã) ã‚’ä¿æŒã—ã¾ã™ã€‚\n",
    "ds_raw = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "ds_raw = ds_raw.train_test_split(test_size=eval_ratio, seed=seed)\n",
    "print(\"train size:\", len(ds_raw[\"train\"]))\n",
    "print(\"eval size:\", len(ds_raw[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b66203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶åˆæœŸåŒ–ï¼ˆUnslothï¼‰\n",
    "# ç›®çš„: Unslothã§4bité‡å­åŒ–ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
    "# ã“ã“ã§GPUã«è¼‰ã‚‹ãŸã‚ã€VRAMæ¶ˆè²»ãŒå¤§ãã„ã‚»ãƒ«ã§ã™ã€‚\n",
    "# max_length ãŒé•·ã„ã¨VRAMä½¿ç”¨é‡ãŒå¢—ãˆã‚‹ç‚¹ã«æ³¨æ„ã€‚\n",
    "# VRAMã«å³ã—ã„å ´åˆã¯ max_length ã‚’ä¸‹ã’ã¦ãã ã•ã„ã€‚\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model,\n",
    "    max_seq_length=max_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"pad_token:\", tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°\n",
    "# ç›®çš„: messages -> ChatMLé¢¨ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
    "# - messages_to_text: å­¦ç¿’ç”¨ï¼ˆå…¨æ–‡ï¼‰\n",
    "# - build_prompt_from_messages: ç”Ÿæˆç”¨ï¼ˆassistantç›´å‰ã¾ã§ï¼‰\n",
    "# - extract_assistant_ref: å‚ç…§å›ç­”ã‚’å–ã‚Šå‡ºã™\n",
    "# å­¦ç¿’ç”¨ (å…¨æ–‡) ã¨ç”Ÿæˆç”¨ (assistantç›´å‰ã¾ã§) ã‚’åˆ†ã‘ã¦ã„ã¾ã™ã€‚\n",
    "def messages_to_text(messages: List[dict]) -> str:\n",
    "    parts = []\n",
    "    for m in messages:\n",
    "        if not isinstance(m, dict):\n",
    "            continue\n",
    "        role = m.get(\"role\", \"user\")\n",
    "        content = m.get(\"content\", \"\")\n",
    "        parts.append(f\"<|{role}|>\\n{content}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def build_prompt_from_messages(messages: List[dict]) -> str:\n",
    "    parts = []\n",
    "    for m in messages:\n",
    "        if not isinstance(m, dict):\n",
    "            continue\n",
    "        role = m.get(\"role\", \"user\")\n",
    "        content = m.get(\"content\", \"\")\n",
    "        if role == \"assistant\":\n",
    "            break\n",
    "        parts.append(f\"<|{role}|>\\n{content}\")\n",
    "    parts.append(\"<|assistant|>\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def extract_assistant_ref(messages: List[dict]) -> str:\n",
    "    for m in messages:\n",
    "        if isinstance(m, dict) and m.get(\"role\") == \"assistant\":\n",
    "            return str(m.get(\"content\", \"\"))\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ãƒ†ã‚­ã‚¹ãƒˆåŒ– + ã‚«ãƒ©ãƒ æ•´å½¢\n",
    "# ç›®çš„: å­¦ç¿’ç”¨ã® text ã‚«ãƒ©ãƒ ã ã‘ã‚’ä½œã‚Šã€Trainerã«æ¸¡ã›ã‚‹å½¢ã«ã—ã¾ã™ã€‚\n",
    "# ã“ã‚Œä»¥é™ã®å­¦ç¿’ã¯ text ã‚«ãƒ©ãƒ ã®ã¿ã‚’ä½¿ã„ã¾ã™ã€‚\n",
    "# ã“ã“ã‹ã‚‰å…ˆã¯ text ã‚’ä½¿ã£ã¦å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "def format_fn(example: Dict[str, object]) -> Dict[str, str]:\n",
    "    messages = example.get(\"messages\", [])\n",
    "    return {\"text\": messages_to_text(messages)}\n",
    "\n",
    "ds = ds_raw.map(format_fn, remove_columns=ds_raw[\"train\"].column_names)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Sanity-checkç”¨ã®å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "# ç›®çš„: æœ¬ç•ªã¨åŒã˜æˆæœç‰©ã‚’çŸ­æ™‚é–“ã§å¾—ã‚‹ãŸã‚ã®å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨æ„ã—ã¾ã™ã€‚\n",
    "# - train/eval ã‚’å°‘æ•°ä»¶ã ã‘æŠ½å‡º\n",
    "# - æœ¬ç•ªã®æµã‚Œã‚’å£Šã•ãšå‹•ä½œç¢ºèªãŒå¯èƒ½\n",
    "sanity_train_size = 64\n",
    "sanity_eval_size = 16\n",
    "\n",
    "sanity_raw_train = ds_raw[\"train\"].select(range(min(sanity_train_size, len(ds_raw[\"train\"]))))\n",
    "sanity_raw_eval = ds_raw[\"test\"].select(range(min(sanity_eval_size, len(ds_raw[\"test\"]))))\n",
    "\n",
    "sanity_ds = {\n",
    "    \"train\": sanity_raw_train.map(format_fn, remove_columns=sanity_raw_train.column_names),\n",
    "    \"test\": sanity_raw_eval.map(format_fn, remove_columns=sanity_raw_eval.column_names),\n",
    "}\n",
    "\n",
    "print(\"sanity train:\", len(sanity_ds[\"train\"]))\n",
    "print(\"sanity eval:\", len(sanity_ds[\"test\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Outputã‚¿ã‚°å‡ºç¾ç‡\n",
    "# ç›®çš„: Outputç³»ã‚¿ã‚°ãŒã©ã®ç¨‹åº¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚\n",
    "# ã“ã“ã§å€¤ãŒä½ã™ãã‚‹ã¨ loss ãƒã‚¹ã‚¯ãŒåŠ¹ãã«ãã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "def has_tag(text: str) -> bool:\n",
    "    return any(tag in text for tag in MASK_TAGS)\n",
    "\n",
    "tag_hits = sum(1 for t in ds[\"train\"][\"text\"] if has_tag(t))\n",
    "print(f\"Tag hit ratio (train): {tag_hits}/{len(ds['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf30c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ãƒˆãƒ¼ã‚¯ãƒ³é•·åˆ†å¸ƒã®ä¿å­˜\n",
    "# ç›®çš„: tokené•·åˆ†å¸ƒã‚’ä¿å­˜ã—ã€max_lengthã®å¦¥å½“æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
    "# ä½µã›ã¦ run_meta.yaml ã«å­¦ç¿’æ¡ä»¶ã‚’è¨˜éŒ²ã—ã¾ã™ã€‚\n",
    "# run_meta.yaml / length_stats.yaml ãŒ outputs ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚\n",
    "def percentile(values: List[int], p: float) -> int:\n",
    "    if not values:\n",
    "        return 0\n",
    "    idx = int(round((p / 100.0) * (len(values) - 1)))\n",
    "    return values[idx]\n",
    "\n",
    "def compute_length_stats(texts: List[str], tokenizer, max_length: int) -> Dict[str, int]:\n",
    "    lengths: List[int] = []\n",
    "    for t in texts:\n",
    "        ids = tokenizer(t, add_special_tokens=False, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "        lengths.append(len(ids))\n",
    "    lengths.sort()\n",
    "    return {\n",
    "        \"samples\": len(lengths),\n",
    "        \"p50\": percentile(lengths, 50),\n",
    "        \"p75\": percentile(lengths, 75),\n",
    "        \"p90\": percentile(lengths, 90),\n",
    "        \"p95\": percentile(lengths, 95),\n",
    "        \"p97\": percentile(lengths, 97),\n",
    "        \"p98\": percentile(lengths, 98),\n",
    "        \"p99\": percentile(lengths, 99),\n",
    "        \"p99_5\": percentile(lengths, 99.5),\n",
    "        \"max\": lengths[-1] if lengths else 0,\n",
    "    }\n",
    "\n",
    "def save_yaml(path: Path, payload: Dict[str, object]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(yaml.safe_dump(payload, allow_unicode=True), encoding=\"utf-8\")\n",
    "\n",
    "run_meta = {\n",
    "    \"base_model\": base_model,\n",
    "    \"data_path\": data_path,\n",
    "    \"eval_ratio\": eval_ratio,\n",
    "    \"seed\": seed,\n",
    "    \"max_length\": max_length,\n",
    "    \"batch_size\": int(train_cfg[\"batch_size\"]),\n",
    "    \"grad_accum\": int(train_cfg[\"grad_accum\"]),\n",
    "    \"learning_rate\": float(train_cfg[\"learning_rate\"]),\n",
    "    \"epochs\": float(train_cfg[\"epochs\"]),\n",
    "    \"precision\": str(train_cfg[\"precision\"]),\n",
    "    \"packing\": bool(train_cfg[\"packing\"]),\n",
    "    \"lora_r\": int(lora_cfg[\"r\"]),\n",
    "    \"lora_alpha\": int(lora_cfg[\"alpha\"]),\n",
    "    \"lora_target_modules\": list(lora_cfg[\"target_modules\"]),\n",
    "    \"quantization\": {\n",
    "        \"load_in_4bit\": True,\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",\n",
    "        \"bnb_4bit_use_double_quant\": True,\n",
    "        \"bnb_4bit_compute_dtype\": \"bf16\",\n",
    "    },\n",
    "    \"mask_tags\": MASK_TAGS,\n",
    "    \"tag_hit_train\": tag_hits,\n",
    "    \"train_size\": len(ds[\"train\"]),\n",
    "    \"eval_size\": len(ds[\"test\"]),\n",
    "}\n",
    "\n",
    "length_stats = compute_length_stats(ds[\"train\"][\"text\"], tokenizer, max_length=max_length)\n",
    "save_yaml(Path(output_dir) / \"run_meta.yaml\", run_meta)\n",
    "save_yaml(Path(output_dir) / \"length_stats.yaml\", length_stats)\n",
    "\n",
    "print(\"Saved run_meta.yaml and length_stats.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. LoRA è¨­å®š + gradient checkpointingï¼ˆUnslothï¼‰\n",
    "# ç›®çš„: LoRAã‚’æœ‰åŠ¹åŒ–ã—ã€Unslothã®gradient checkpointingã§VRAMç¯€ç´„ã—ã¾ã™ã€‚\n",
    "# ã“ã“ã§ LoRA ã®ãƒ©ãƒ³ã‚¯(r)ã‚„alphaãŒåæ˜ ã•ã‚Œã¾ã™ã€‚\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=int(lora_cfg[\"r\"]),\n",
    "    lora_alpha=int(lora_cfg[\"alpha\"]),\n",
    "    target_modules=list(lora_cfg[\"target_modules\"]),\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=seed,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "print(\"LoRA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfe4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. labels ã‚’ Outputã‚¿ã‚°ä»¥é™ã«é™å®šã™ã‚‹ collator\n",
    "# ç›®çš„: Outputã‚¿ã‚°ä»¥é™ã®ã¿ loss ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®collatorã‚’ä½œã‚Šã¾ã™ã€‚\n",
    "# - ã‚¿ã‚°ãŒç„¡ã„å ´åˆã¯å…¨æ–‡ã‚’losså¯¾è±¡ã«ã—ã¾ã™ã€‚\n",
    "def find_first_tag_pos(text: str, tags: List[str]) -> Optional[Tuple[int, str]]:\n",
    "    found = []\n",
    "    for tag in tags:\n",
    "        idx = text.find(tag)\n",
    "        if idx >= 0:\n",
    "            found.append((idx, tag))\n",
    "    if not found:\n",
    "        return None\n",
    "    found.sort(key=lambda x: x[0])\n",
    "    return found[0]\n",
    "\n",
    "@dataclass\n",
    "class CollatorConfig:\n",
    "    tokenizer: object\n",
    "    max_length: int\n",
    "    tags: List[str]\n",
    "\n",
    "class OutputTagCollator:\n",
    "    def __init__(self, cfg: CollatorConfig) -> None:\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, object]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = [f[\"text\"] for f in features]\n",
    "        tok = self.cfg.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.cfg.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = tok[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "        for i, text in enumerate(texts):\n",
    "            hit = find_first_tag_pos(text, self.cfg.tags)\n",
    "            if hit is None:\n",
    "                continue\n",
    "            tag_pos, tag = hit\n",
    "            prefix = text[: tag_pos + len(tag)]\n",
    "            prefix_ids = self.cfg.tokenizer(\n",
    "                prefix, truncation=True, max_length=self.cfg.max_length, add_special_tokens=False\n",
    "            )[\"input_ids\"]\n",
    "            cut = len(prefix_ids)\n",
    "            labels[i, :cut] = -100\n",
    "        tok[\"labels\"] = labels\n",
    "        return tok\n",
    "\n",
    "collator = OutputTagCollator(\n",
    "    CollatorConfig(tokenizer=tokenizer, max_length=max_length, tags=MASK_TAGS)\n",
    ")\n",
    "print(\"collator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6703519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆãƒ­ã‚°\n",
    "# ç›®çš„: å„ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«å›ºå®šã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã€ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã™ã€‚\n",
    "# - äººé–“ãŒè¦‹ã¦æ”¹å–„ç‚¹ã‚’åˆ¤æ–­ã§ãã‚‹å‡ºåŠ›ã‚’æ®‹ã—ã¾ã™ã€‚\n",
    "# äººé–“ãŒè¦‹ã¦æ”¹å–„ç‚¹ã‚’åˆ¤æ–­ã§ãã‚‹ãƒ­ã‚°ã«ãªã‚Šã¾ã™ã€‚\n",
    "class EvalSampleCallback(TrainerCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        output_dir: str,\n",
    "        sample_size: int,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float,\n",
    "        top_p: float,\n",
    "        seed: int,\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.sample_size = sample_size\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        rng = torch.Generator().manual_seed(seed)\n",
    "        n = len(dataset)\n",
    "        if n == 0:\n",
    "            self.indices = []\n",
    "        else:\n",
    "            perm = torch.randperm(n, generator=rng).tolist()\n",
    "            self.indices = perm[: min(sample_size, n)]\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        model_was_training = model.training\n",
    "        model.eval()\n",
    "\n",
    "        results = []\n",
    "        device = next(model.parameters()).device\n",
    "        for idx in self.indices:\n",
    "            ex = self.dataset[idx]\n",
    "            messages = ex.get(\"messages\", [])\n",
    "            prompt = build_prompt_from_messages(messages)\n",
    "            ref = extract_assistant_ref(messages)\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, return_tensors=\"pt\", add_special_tokens=False\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=self.temperature,\n",
    "                    top_p=self.top_p,\n",
    "                )\n",
    "            text = self.tokenizer.decode(out[0], skip_special_tokens=False)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"epoch\": float(state.epoch),\n",
    "                    \"index\": idx,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"prediction\": text,\n",
    "                    \"reference\": ref,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        out_path = self.output_dir / f\"eval_samples_epoch{int(state.epoch)}.jsonl\"\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for r in results:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if model_was_training:\n",
    "            model.train()\n",
    "\n",
    "print(\"EvalSampleCallback ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17eb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. TrainingArguments\n",
    "# ç›®çš„: å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®è©³ç´°è¨­å®šã‚’è¡Œã„ã¾ã™ã€‚\n",
    "# - eval_steps / save_steps / precision ãªã©\n",
    "# - report_to=\"wandb\" ã§W&Bã«ãƒ­ã‚°é€ä¿¡\n",
    "# WandB ã® run_name ã‚‚ã“ã“ã§æ¸¡ã—ã¾ã™ã€‚\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=float(train_cfg[\"epochs\"]),\n",
    "    per_device_train_batch_size=int(train_cfg[\"batch_size\"]),\n",
    "    per_device_eval_batch_size=int(train_cfg[\"batch_size\"]),\n",
    "    gradient_accumulation_steps=int(train_cfg[\"grad_accum\"]),\n",
    "    learning_rate=float(train_cfg[\"learning_rate\"]),\n",
    "    save_steps=int(train_cfg[\"save_steps\"]),\n",
    "    eval_steps=int(train_cfg[\"eval_steps\"]),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    bf16=str(train_cfg[\"precision\"]).lower() == \"bf16\",\n",
    "    fp16=str(train_cfg[\"precision\"]).lower() == \"fp16\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "print(\"TrainingArguments ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5 Sanity-check ç”¨ã® TrainingArgumentsï¼ˆè»½é‡ï¼‰\n",
    "# ç›®çš„: Sanityç”¨ã®è»½é‡è¨­å®šã‚’ç”¨æ„ã—ã¾ã™ã€‚\n",
    "# - stepsã‚’æœ€å°åŒ–ã—ã¦é«˜é€Ÿã«çµ‚äº†ã•ã›ã‚‹\n",
    "sanity_output_dir = str(PROJECT_ROOT / train_cfg[\"output_dir\"] / (\"sanity-\" + run_name))\n",
    "\n",
    "sanity_args = TrainingArguments(\n",
    "    output_dir=sanity_output_dir,\n",
    "    num_train_epochs=1.0,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=float(train_cfg[\"learning_rate\"]),\n",
    "    save_steps=1,\n",
    "    eval_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    bf16=str(train_cfg[\"precision\"]).lower() == \"bf16\",\n",
    "    fp16=str(train_cfg[\"precision\"]).lower() == \"fp16\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"sanity-\" + run_name,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# sanity ç”¨ã®ãƒ¡ã‚¿æƒ…å ±ã‚‚ä¿å­˜\n",
    "sanity_meta = dict(run_meta)\n",
    "sanity_meta[\"train_size\"] = len(sanity_ds[\"train\"])\n",
    "sanity_meta[\"eval_size\"] = len(sanity_ds[\"test\"])\n",
    "\n",
    "save_yaml(Path(sanity_output_dir) / \"run_meta.yaml\", sanity_meta)\n",
    "save_yaml(Path(sanity_output_dir) / \"length_stats.yaml\", length_stats)\n",
    "\n",
    "print(\"Sanity TrainingArguments ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Trainer ä½œæˆ\n",
    "# ç›®çš„: æœ¬ç•ªå­¦ç¿’ç”¨ã®Trainerã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    formatting_func=lambda x: x[\"text\"],\n",
    "    data_collator=collator,\n",
    "    args=args,\n",
    "    packing=bool(train_cfg[\"packing\"]),\n",
    "    max_seq_length=max_length,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer.add_callback(\n",
    "    EarlyStoppingCallback(\n",
    "        early_stopping_patience=int(train_cfg[\"early_stopping_patience\"]),\n",
    "        early_stopping_threshold=float(train_cfg[\"early_stopping_threshold\"]),\n",
    "    )\n",
    ")\n",
    "trainer.add_callback(\n",
    "    EvalSampleCallback(\n",
    "        dataset=ds_raw[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=output_dir,\n",
    "        sample_size=int(train_cfg[\"eval_sample_size\"]),\n",
    "        max_new_tokens=int(train_cfg[\"eval_max_new_tokens\"]),\n",
    "        temperature=float(train_cfg[\"eval_temperature\"]),\n",
    "        top_p=float(train_cfg[\"eval_top_p\"]),\n",
    "        seed=seed,\n",
    "    )\n",
    ")\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.5 Sanity-check Trainer\n",
    "# ç›®çš„: Sanityç”¨Trainerã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "sanity_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=sanity_ds[\"train\"],\n",
    "    eval_dataset=sanity_ds[\"test\"],\n",
    "    formatting_func=lambda x: x[\"text\"],\n",
    "    data_collator=collator,\n",
    "    args=sanity_args,\n",
    "    packing=bool(train_cfg[\"packing\"]),\n",
    "    max_seq_length=max_length,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "sanity_trainer.add_callback(\n",
    "    EarlyStoppingCallback(\n",
    "        early_stopping_patience=1,\n",
    "        early_stopping_threshold=0.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "sanity_trainer.add_callback(\n",
    "    EvalSampleCallback(\n",
    "        dataset=sanity_raw_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=sanity_output_dir,\n",
    "        sample_size=min(4, len(sanity_raw_eval)),\n",
    "        max_new_tokens=int(train_cfg[\"eval_max_new_tokens\"]),\n",
    "        temperature=float(train_cfg[\"eval_temperature\"]),\n",
    "        top_p=float(train_cfg[\"eval_top_p\"]),\n",
    "        seed=seed,\n",
    "    )\n",
    ")\n",
    "print(\"Sanity Trainer ready\")\n",
    "\n",
    "# 12.6 Sanity-check å®Ÿè¡Œï¼ˆè»½é‡ï¼‰\n",
    "# ç›®çš„: æœ¬ç•ªã¨åŒã˜æˆæœç‰©ï¼ˆadapter/ãƒ­ã‚°/è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«ï¼‰ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚\n",
    "sanity_trainer.train()\n",
    "sanity_trainer.save_model(sanity_output_dir)\n",
    "tokenizer.save_pretrained(sanity_output_dir)\n",
    "print(\"Saved sanity outputs to:\", sanity_output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".maLM25main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
